%************************************************
\chapter{Introduction and preliminaries}
\label{chp:introduction}
%\minitoc\mtcskip
%************************************************

The study of the language of the papyri has in the past thirty years
seen little evolution until the recent appearance of Evans and Obbink's
\textit{The Language of the Papyri} \citep{lpapyri}, which has placed
the subject in the spotlight again. Twentieth-century scholarship on the
topic, though still useful for those interested in the study of the
papyri for historical purposes, is either antiquated, limited in scope
or incomplete (see \textit{infra}). Despite this, the papyri are
useful source material for the history and evolution of the Greek
language, as they contain not only official texts but private
documents as well, whose linguistic features and peculiarities have the
potential to foster new insights into the nature of colloquial Greek. 

\section{Thesis}
The following thesis intends to prove that it is possible to generate
basic linguistic annotation for a large digitalised corpus of papyri in
ancient Greek using readily available tools and techniques with minimal
technical overhead. Such a corpus could be  a boon to scholars interested
in the Greek of the papyri, as it would facilitate, for instance, the
creation of linguistically sound grammars and lexica.

\section{Preliminaries}

The following section will provide a background sketch, consisting of a short
overview of previous efforts and an elucidation of some key concepts.

\subsection{The language of the papyri}

The papyri began to be studied linguistically not by papyrologists and
historians, but rather by Bible scholars and grammarians interested in their
relevance in the development began to koin\^{e} Greek, particularly that of the
New Testament. G. N.  Hatzidakis, W. Cr\"onert, K. Dieterich, A. Deissmann, and
A.  Thumb pioneered the field in the late nineteenth and early twentieth
century, spurring a resurgence of scholarship on the topic; an excellent
overview of pre-1970s research may be found in \citet{mandilaras1973} and
\citeauthor{gignac1976} [\citeyear{gignac1976} and \citeyear{gignac1981}].

During this period, Mayser begon work on the earliest compendious grammar of
the papyri; it limits itself to the Ptolemaic era but explores it at length and
in great detail.  The work consists of a part on phonology and morphology, made
up of three slimmer volumes, and a part on syntax, encompassing three larger
volumes. Its composition seems to have been exhausting: it took Mayser
thirty-six years to finish volumes I.2 through II.3, with I.1 only completed in
1970 by Hans Schmoll, at which point the entire series was given a second
edition.

When casually browsing through some of its chapters (though casual is hardly
the word one would associate with the \textit{Grammatik}) it is remarkable to see
that Mayser brings an abundance of material to the table for each grammatical
observation he makes, however small it may be. For instance, the section on
diminutives essentially consists of pages upon pages of examples categorised by
their endings.

This is its great strength as a reference work - whenever one is faced with an
unusual grammatical phenomenon in any papyrus, consulting Mayser is bound to
clarify the matter; or rather, it was, for the work is now inevitably dated.
The volumes published during Mayser's lifetime only include papyri up to their
date of publication; only the first tome by Schmoll includes papyri up to 1968.
It is still a largely useful resource, but it is in urgent need of refreshment.

After Mayser set the standard for the Ptolemaic papyri, a grammar of the
post-Ptolemaic papyri was the new \textit{desideratum} in papyrology. The work
had been embarked on by Salonius, Ljungvik, Kapsomenos, and Palmer, only to be
interrupted or thwarted by circumstance or lack of resources.
\citet{salonius1927}, for instance, only managed to write an introduction on
the sources, though he offered valuable comments on the matter of deciding how
close to spoken language a piece of writing is. \citet{ljungvik1932} contains
select studies on some points of syntax.

It is in the 1930's that we see attempts to create a grammar of the papyri that
would be the equivalent of Mayser for the post-Ptolemaic period.
\citeauthor{kapsomenos1938} published a series of critical notes
[\citeyear{kapsomenos1938}, \citeyear{kapsomenos1957}] on the
subject; though he attempted at a work on the scale of the \textit{Grammatik},
he found the resources sorely lacking, as the existing editions of papyrus
texts could not form the basis for a systematic grammatical study. The other
was \citeauthor{palmer1934}, who had embarked on similar project and had
already set out a methodology [\citeyear{palmer1934}]; the war interrupted his
efforts, and he published what he had already completed, a treatise on the
suffixes in word formation [\citeyear{palmer1945}].

A new work of some magnitude presents itself two decades later with B. G.
Mandilaras' \textit{The verb in the Greek non-literary papyri}
[\citeyear{mandilaras1973}]. Though it does not aim to be a grammar of the
papyri, it does offer a thorough and satisfactory treatment of the verbal
system as manifest in the papyri.  Further efforts essentially do not appear
until the publication of Gignac's grammar. It is essentially treading in the
footsteps of Mayser, only with further methodological refinement and a more
limited, though still sufficiently exhaustive, array of examples. The author,
for reasons unknown to me, only managed to complete two of the three projected
volumes, on phonology and on morphology. The volume on syntax is thus absent, a
gap only partly filled by Mandilaras' \textit{The verb in the Greek
non-literary papyri}.

Finally, there is the aforementioned \textit{The Language of the Papyri}
\citep{lpapyri}, which does not aim to be a work on the same scale as the
aforementioned. It is a collection of articles on various topics, the whole of
which is meant to illuminate new avenues for future research. A particularly
relevant chapter for this thesis is the last one by Porter and O'Donnell
\citep{porter2010}, who set out to create a linguistic corpus for a selection
of papyri; their tagging approach, however, is manual, and their target corpus
limited. The authors also are the creators of \url{http://www.opentext.org/}, a
project aiming for the development of annotated Greek corpora and tools to
analyse them; sadly, no progress seems to have been made since 2005.

\subsection{Corpus linguistics}
A\footnote{The following section is based \emph{passim} on
\citet{okeeffe2010}.} corpus or text corpus is a large, structured collection
of texts designed for the statistical testing of linguistic hypotheses. The
core methodological concepts of this mode of analysis may be found in the
concordance, a tool first created by biblical scholars in the Middle Ages as an
aid in exegesis. Among literary scholars, the concordance also enjoyed use,
although to a lesser degree; the eighteenth century saw the creation of a
concordance to Shakespeare.

 The development of the concordance into the modern corpus was not primarily
 driven by the methods of biblical and literary scholars; rather, lexicography
 and pre-Chomskyan structural linguistics played a crucial role.

 Samuel Johnson created his famous comprehensive dictionary of English by means
 of a manually composed corpus consisting of countless slips of paper detailing
 contemporary usage. A similar method was used in the 1880s for the Oxford
 English Dictionary project - a staggering three million slips formed the basis
 from which the dictionary was compiled.

 1950s American structuralist linguistics was the other prong of progress; its
 heralding of linguistic data as a central given in the study of language
 supported by the ancient method of searching and indexing ensures its
 proponents may be called the forerunners of corpus linguistics.

Computer-generated concordances make their appearance in the late 1950s,
initially relying on the clunky tools of the day - punch cards. A notable
example is the \emph{Index Thomisticus}, a concordance
to the works of Thomas of Aquino created by the late Roberto Busa S.J. which
only saw completion after thirty years of hard work; the printed version spans
56 volumes and is a testament to the diligence and industry of its author. The
1970s brought strides forward in technology, with the creation of computerised
systems to replace catalogue indexing cards, a change that greatly benefited
bibliography and archivistics.

 It is only in the 1980s and 1990s that are marked the arrival of fully
 developed corpora in the modern sense of the word; for though the basic
 concepts of corpus linguistics were already widely used, they could not be
 applied on a large scale without the adequate tools. The rise of the desktop
 computer and the Internet as well as the seemingly ever-rising pace of
 technological development ensured the accessibility of digital tools.  The old
 tools - punch cards, mainframes, tape recorders and the like - were gladly
 cast aside in favour of the new data carriers.

 The perpetual increase of computing power equally demonstrated the limits of
 large-scale corpora; while lexicographical projects that had as their purpose
 to document the greatest number of possible usages could keep increasing the
 size of their corpora, the size of others went down as they whittled the data
 down to a specific set of uses of language.

 The possible applications of the techniques of corpus linguistics are diverse
 and numerous; for they allow for a radical enlargement in scope while
 remaining empirical, and remove arduous manual labour from the equation.
 Corpus linguistics can be an end to itself; it can, however, assert an
 important role in broader research.  \citet[7]{okeeffe2010} mention areas such
 language teaching and learning, discourse analysis, literary stylistics,
 forensic linguistics, pragmatics, speech technology, sociolinguistics and
 health communication, among others.

The term `corpus' has a slightly different usage in classical philology: they
designate a structured collection of texts, but that collection is not
primarily intended for the testing of linguistic hypotheses. Instead, we have,
for instance, the ancient corpus Tibullianum, or modern-day collection, for
  instance the Corpus Papyrorum Judaicarum, etc. We are primarily interested in
  the digital techniques used to create linguistic corpora; so let us first
  take a look at the progress of the digital classics.


\subsection{The digital classics} 

Classical philology, despite its status as one of the oldest and most
conservative scientific disciplines still in existence today, has in the last
fifty years found itself at the front lines of the digital humanities movement.
Incipient efforts in the fifties and sixties, mainly stylometric and lexical
studies and  the development of concordances, demonstrated the relevance of
informatics in the classics, an evolution that was at first met with some
skepticism, but later fully embraced.

The efforts began with the aforementioned Index Thomisticus, the first
computer-based corpus in a classical language; but the first true impetus was
the foundation of the Thesaurus Linguae Graecae project in 1972, a monumental
project with as its goal the stocking of all Greek texts from the Homeric epics
to the fall of Constantinople. Over the years, many functions have been added
to this ever more powerful tool; and even in the beginning stages of its
development, the TLG garnered praise.

The usefulness of the tool in its current form cannot be overstated: not only
does it contain a well-formatted and easily accessible gigantic collection of
text editions whose scope and dimensions exceed those of nearly any university
library; it also offers all of these texts in a format that allows for lexical,
morphological and proximity searches, as well as including a full version of
the Liddell \& Scott and Lewis \& Short dictionaries. The TLG has become a
staple of the digital classics.

Despite this, the TLG is becoming more and more dated as technology progresses.
While recent years have seen the rise of Unicode as the standard for encoding
ancient Greek, the TLG still uses beta code, a transliteration system designed
to only use the ASCII character set, and the texts are stored using an obsolete
text-streaming format from 1974, which divides the text in blocks of eight
kilobytes and marks the division between segment. 

A digitised version of the Liddell-Scott-Jones lexicon has been added to the
TLG's web interface, but the texts themselves have not undergone extensive
tagging, only lemmatisation.  Searching through the database can be done by
searching for specific forms of a lemma, or by searching for all forms of a
lemma, but this is essentially the limit of the search tool's power; it is not
possible to perform a query for all possible lemmata associated with a
particular form, i.e.\ we cannot find all forms which are, for example, an
active perfect indicative.

In the wake of the TLG, several notable projects have emerged: Brepols' Library
of Latin Texts is trying hard to be for Latin texts what the TLG is for Greek
texts; the Packard Humanities Institute has released CD's containing a
selection of classical Latin works. In more recent times, the Perseus Project
has enjoyed great popularity because of the attractive combination of an
excellent selection of classical texts with translations, good accessibility
and a set of interesting textual tools, the entire package carrying a very
interesting price tag for the average user — it is free to use, and for the
greatest part, open source as well.

The databases I have mentioned are quite general in scope; but within the
domain of classical philology, other specialised projects exist. Within the
field of papyrology, for instance, the digital revolution has taken a firm
foothold. Starting with several separate databases, the field has experienced a
tendency towards convergence and integration of the available resources, as
exemplarised by the papyri.info website, maintained by Columbia University,
that integrates the main papyrological databases into a single database.

A great feature of this database is the shell in which all data is wrapped;
they are compliant with the EpiDoc standard, a subset of XML based on the TEI
standard and developed specifically for epigraphical and papyrological texts.
One may access the database’s resources through the Papyrological Navigator and
suggest corrections and readings through the Papyrological Editor. What’s more,
all data is freely accessible under the Creative Commons License,
crowd-sourced, regularly updated, and can be downloaded for easier searching
and tweaking.

In other words, papyri.info has brought the open-source mentality from the
computer world into the classics. For our purposes, this open setup is
desirable, as the database is not fit for them as it is, but can with some
effort be molded into a useful tool.

\subsection{Natural language processing} 

Natural language processing (henceforth NLP) is a subdiscipline in computer
science concerned with the interaction between natural human language and
computers. Its history well and truly starts in the fifties, with a basic
concept which has played a great role in natural language processing, and
computer science in general, the Turing test. This test, put forth by Alan
Turing in his seminal paper \textit{Computing Machinery and Intelligence}
\citep{turing1950}, evaluates whether a machine is intelligent or not by
placing a human in conversation with another human and a machine; if the first
human cannot tell the other human and the machine apart, the machine passes the test.

Machine translation systems entered development, though progress soon stalled
because of technical limitations and because of methodological obstacles: such
systems were dependent on complex rulesets written by programmers that allowed
for very little flexibility. Because of the slow return on investments made,
funding for artificial intelligence in general and machine translation
specifically was drastically reduced throughout the late sixties and the seventies.

A resurgence followed: thanks to advances in computational power and the
decline of Chomskyan linguistics, which had been the dominant theoretical
vantage point in the preceding thirty years, the eighties were marked by the
introduction of statistical machine translation, which is fundamentally based
on the tenets of corpus linguistics. Modern natural language processing is
therefore situated on the crossroads between various fields: artificial
intelligence, computer science, statistics, and corpus and computational
linguistics. It looks to be an exciting field for the coming years as its
techniques are under constant improvement and ever more present in our daily lives.

Most NLP software is designed explicitly with living languages in mind;
English, being a world language and the international \textit{lingua franca},
has enjoyed most of the attention, but other major languages have enjoyed some
attention, too. Ancient languages, however, are neglected, presumably due to
their often high complexity and the extensive study and analysis to which they
have been submitted by skilled scholars. Yet most texts have not been
integrated in annotated corpora; and though databases such as the Perseus
project contain large swathes of morphologically and sometimes syntactically
annotated text, the process has been driven largely by manual labour; to give
an exhaustive list is not appropriate here, but another such example which is
relevant is the PROIEL project, which is also a treebank, i.e.\ a database of
syntactically annotated sentences. It contains data for Herodotus and the New Testament.

On the other hand, there are also corpora which have been tagged using NLP
techniques, whose relevancy for this thesis is high and that I have thus
described in the next section on methodology.

\section{Methodology}

In this thesis, we have been largely inspired by two articles by H. Dik and R.
Whaling, [\citeauthor{dik2008}, \citeyear{dik2008} and \citeyear{dik2009}], in
which they document their method for semi-automatically tagging the Perseus
project's texts under their own framework, PhiloLogic. They start with a
database of analysed forms and a series of tagged texts which they use as
initial data to train a decision tree tagger, TreeTagger, developed by Helmut
Schmid at the University of Stuttgart, a tool which despite being developed in
1995 has aged well as far as performance is concerned. They achieved remarkable
accuracy: with refinements to the training data they achieved 96.2\% accuracy
during tests on the original training data and 91\% accuracy on new data, a
result which compares quite favorably when compared to TreeTagger's 97\%
accuracy when used on German newspaper articles considering the high complexity
of ancient Greek and the variety of styles of ancient Greek literature.

It occurred to me that this might be a great method for processing the
papyri.info database with a relatively small effort for a high payoff; using
data from the Perseus and PROIEL projects, it could be possible to train
TreeTagger for both morphology and syntax, apply the resulting parameters to
the corpus and thus for the greatest part obviate the need for manual tagging.
Given the extent of the corpus (about 50,000 texts containg almost 4,500,000
words), achieving even 85\% accuracy would reduce the amount of untagged words
to 675,000, many of which I would expect to be proper names or morphologically
`erroneous' forms as are often found in the papyri, data which could itself be
analysed with regular expressions and then used to improve the training
data.\footnote{As I set out to verify the originality of my thesis, I found
  that this statistical approach has been used before for textual criticism!
  \textit{Vide} \citet{mimno2009}, an abstract of which may be found at
  \url{http://people.cs.umass.edu/~wallach/publications/mimno09computational.txt}.}

