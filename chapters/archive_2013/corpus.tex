%************************************************
\chapter{The corpus}
\label{chp:design}
\minitoc\mtcskip
%************************************************
\section{Goals}

Our objectives for the modified corpus is threefold. Firstly, we want to
morphologically annotate it with reasonable accuracy; secondly, we also want to
add syntactical annotation; thirdly, we want to ensure the corpus is compatible
with a broad range of tools.

The former two goals are the central enterprise of this thesis. Morphological
annotation will pose little theoretical problems, as the morphological features
of Greek are not subject to discussion; but for syntactical annotation, a
variety of theoretical frameworks are in existence. The fact of the matter is
that I have chosen to take the route which is technically easiest: to use the
tenets of dependency grammar. The reason is twofold: on one hand, our training
data, the New Testament annotated by PROIEL, itself is structured using a
dependency model; on the other hand, dependency trees, in contrast to
constituent analysis, assigns each word or morph one node, which is easier to
`digest' for a computer program.

Our third goal is to format the corpus in a way that will ensure broad
compatibility. This is desirable for several reasons. A first is that a broadly
used format offers many options for data treatment; for instance,
comma-separated text files can be read by many programs, from the simplest text
editor to the most complex software packages, while XML can very easily be
integrated in a web interface or parsed and transformed by a variety of
programs. A second is that the corpus is to be published on GitHub for
collaborative editing, or possibly pulled into the main papyri.info repository.
This depends on our first point: the idea of GitHub is that all projects are
open source and can be contributed to by others, which is facilitated by using
a format with wide adoption.

\section{Design}
\subsection{Initial inquiries}

Before arriving at the approach described below, I attempted several other
possible methods for automatic analysis. A first idea was to integrate the XML
files in a PhiloLogic setup.  PhiloLogic is a tool developed by the ARTFL
project and the Digital Library Development center at the University of Chicago
and released under the GNU General Public License. Its original purpose was to
serve as the full-text search, retrieval and analysis tool for large
databases of French literature.

PhiloLogic has support for TEI XML and boasts an impressive
array of features: it can search through text and deliver KWIC results
filtered by frequency and metadata, as well as collocation tables and word
order information.  Furthermore, the development web page cites support for
bibliographic backends in MySQL databases, out of the box operation,
interoperability across Unix-based systems, etc.

The tool seemed promising; especially the built-in support for XML and
Unicode drew my attention, along with the built-in features for linguistic
analysis. It essentially would have made my work easier and allowed me to
invest more time in investigating a few linguistic topics. In the end, however,
the software did not fit my needs in a few respects.

Firstly, PhiloLogic requires Apache, Perl and several CPAN modules, as well as
the utilities gawk, gdbm, and agrep.  Not a huge amount of dependencies, but
still less than using Python and the NLTK, which does not require a server or
SQL database to run; also a big problem is the out-of the box incompatibility
of the required CPAn  modules with 64-bit systems. Setting PhiloLogic up was
not as easy as advertised; I had to resort to a virtual machine running 32-bit
Debian Linux until I was able to discover a method that enabled compatibility
with Mac OS X 10.7, my OS of choice.

Secondly, XML support in PhiloLogic includes standard TEI  but has issues with
EpiDoc; notably, in my experience, headers were treated as text and EpiDoc's
complicated tagset was incorrectly rendered; for instance, original readings
and corrections were concatenated in the text browser, an undesirable situation
if one wishes to have any hand in our choice of corpus, and a possible source of
statistical errors.  Developing brand new XSLT stylesheets to convert EpiDoc to
a simpler form of TEI markup or to raw text could perhaps have been a feasible
solution; but PhiloLogic seemed to behave erratically in general and a first
attempt at feeding the system raw text (advertised as one of its capabilities)
turned out a spathe of errors.

Thirdly, PhiloLogic's feature set is extensive, but I soon came to realize that it
does not offer much more than papyri.info already does; it is essentially a
robust concordancing application through a web interface. I wished not only to
have a lemmatised corpus searchable by word proximity, but also by word
relations, which would shed more light on the syntax of the Greek of the
papyri, as this is arguably the largest gap in scholarship due to the age of
Mayser's \emph{Grammatik} and the incomplete state of Gignac's \emph{Grammar}.
Furthermore, the tool as available to the public does not integrate the
morphological database of Greek developed for Perseus under PhiloLogic --- a
regrettable point, really, since it was exactly this capacity I wished to
exploit in the first place. Unicode search for Greek also seems to be in need
of improvement; the search form still requires transcription. Modifying the
tool would have been unfeasible given the size of the source code.

Thus I discarded PhiloLogic from my options and set out to find or develop a
simpler system.\footnote{I also considered setting up a mirror of papyri.info
on a personal server and modifying the search functionality; this turns out not
to be a task for the weak-hearted. The required setup causes substantial
overhead for our purposes, too much to be a reasonable solution.} After the
technical struggle with PhiloLogic, I decided to focus my efforts on
concordancers that could replicate its functions. A quick consultation of
Stanford's list of resources \citep{stanfordnlpres} for statistical natural
language processing and corpus-based computational linguistics directed me to
WordSmith Tools, which is regrettably Windows-only and a commercial, closed
source program to boot.  I set out on a search for open-source alternatives and
found a diversity of programs, most of them without the same functionality as
WordSmith, and invariably clunky or antiquated --- making WordSmith the only
option, but even that did not seem viable for my ends.

I continued my search and stumbled across a very interesting piece of software
that is enjoying a good amount of popularity as a didactic tool for
computational corpus linguistics; the Python NLTK, short for Natural
Language Tool Kit, is not a stand-alone application, but rather a set of ``open
source Python modules for research and development in natural language
processing and text analytics bundled with data and documentation''
\citep{nltkhome}. These modules implement diverse functionalities useful for
natural language processing and allow their easy integration into Python
programs. The Python NLTK  seemed to offer a more than solid amount of
features: corpus readers, tokenizers, stemmers, taggers, chunkers, parsers,
classifiers, clusterers, tools for semantic interpretation and metrics were all
integrated from the get-go and are easily combined and extended. The
NLTK was developed for English, but its extensibility meant it could
be applied to ancient Greek with relative ease. A further advantage is the fact
that Python is very well-suited to text manipulation and parsing XML,
which is an advantage when working with the papyri.info corpus; it is also
cross-platform: Python interpreters are available for all major platforms.

Now that I had a solid tool for interpreting corpora, the only issue remaining
was to construct a linguistically annotated corpus out of the papyri.info
corpus. The extent of the corpus makes extensive manual tagging by a single
person unfeasible; thus I set out to look for an automated tagger which could
be used, immediately or with some effort,  on ancient Greek. A variety of
taggers seem to be available, but I settled on TreeTagger, developed by Helmut
Schmid, for several reasons:

\section{Technical} % (fold)
\label{sec:corpus-technical}

\subsection{Requirements} % (fold)
\label{sub:requirements}

To create the annotated corpus as I have, it is necessary to have the following
installed (older versions may work but have not been tested):

\begin{itemize}

  \item a UNIX-like operating system, i.e. Mac OS X, any variety of Linux or
    BSD (using Windows should also be possible, since all tools used are
    portable and cross-platform, but the file location hierarchies in some
    scripts will not work);

  \item Python 3.2.3, found at \url{http://www.python.org/} (some scripts
    require Python 2.7.3: they are indicated as such at the top of the relevant
    script), with some additional libraries:

    \begin{itemize}

      \item the Python Natural Language Toolkit, found at
        \url{http://nltk.org/}, which is only compatible with Python 2.5.x to
        2.7.x (sparingly used);

      \item the numpy library, which contains a vast array of mathematical
        functionality, found at \url{http://numpy.scipy.org/}.

    \end{itemize}

  \item Perl 5.x \url{http://www.perl.org/};

  \item Saxon-HE 9.x, found at \url{http://saxon.sourceforge.net/};

  \item Helmut Schmid's TreeTagger, found at
    \url{http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/};

  \item SQLite 3, found at \url{http://www.sqlite.org/}.

\end{itemize}

All the above, excluding Mac OS X, are freely available.

% subsection Requirements (end)


\subsection{Extracting training data} % (fold)
\label{sub:extract_training_data}
To train TreeTagger and the Stanford Parser, we need annotated data that can be analysed by the programs. We have chosen the following dataset:

\begin{enumerate}
  \item For TreeTagger (lemmatisation and morphological tagging):
\begin{itemize}
  \item as a training file: the Greek New Testament and Herodotus' histories, annotated by the PROIEL project, found at \url{http://foni.uio.no:3000/};
  \item as a lexicon: the Perseus project's million-word morphological database (found at \url{http://www.perseus.tufts.edu/hopper/opensource/download} plus all parses from the training file.
\end{itemize}

\item For the Stanford Parser (dependency parsing):
\begin{itemize}
  \item as a training file: the Greek New Testament and Herodotus' histories, annotated by the PROIEL project, supplemented with the Perseus treebanks found at \url{http://nlp.perseus.tufts.edu/syntax/treebank/}.
\end{itemize}
% subsection Extracting training data (end)
\end{enumerate}

There are a few obstacles to unifying and formatting these different
resources; for instance, the PROIEL project and the Perseus project use
different formats; the former has published its data in three different XML
formats, while the latter has made its databases available in SQL dump format.
The PROIEL project has all its Greek text stored in Unicode, while the entire
Perseus framework still relies on beta code, which created a few obstacles
along the road. Furthermore, the annotation schemes for both databases are
different; PROIEL uses ten-characters morphological abbreviations derived from
the Perseus system, with the difference that the Perseus system only use nine
characters and orders them slightly differently. Most of the conversion work
has been automatised by a series of Python scripts, but here and there some
manual input is still required. 


\subsubsection{Morphological data} % (fold)
\label{ssub:morphdata}

% subsubsection Morphological data (end)
% subsection subsection name (end)
A first step is the formatting of the PROIEL morphological database and
treebank. This is done with an XSL transformation stylesheet which outputs the
data in a two raw text files, one to be used as a training corpus, another to
serve as a supplement to the Perseus-based lexicon; a Python script is then run
which sorts the lexicon file and removes duplicate forms. It then converts the
parses in both files to the format used in the Perseus project's databases,
this in order to attain a unified training file with minimum complexity (see:
tables 1, 2 and 3).

A second step is the extraction of the Perseus morphological data, which is a
bit less straightforward. They have published their data in SQL dump format,
that is, raw text file which reflect the structure of the relational database
in a programmatic way so that it can be inserted into a new database. I wrote a
script which created a separate text file for each important relation: lemmata
and their ID's, parses and their ID's, and finally a combined file with every
inflected form followed by the ID of its parse and lemma. I then converted all
Greek text from beta code to Unicode using the Java beta code conversion
utility provided by the EpiDoc consortium (the utility may be found at
\url{http://sourceforge.net/projects/epidoc/files/transcoder/}). Once
everything has been converted to Unicode, a Python script creates a SQLite
database from the data for later use; using its relational functionality, we
then output a final lexicon file formatted for use with TreeTagger and with all
Greek text in Unicode.

The third step is unification and final formatting; we merge both lexica into a
single text file, sort the contents and remove all duplicates, after which
another script is run which aligns all possible parses on one line, in this
way:

\lstset{numbers=left,
  escapebegin=\Greek,
  escapeend=\Latin,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt
}

\begin{lstlisting}[firstnumber=1, escapeinside='']
'ἀγαπᾶτε'	2ppia----i	'ἀγαπάω',V-
'ἀγαπᾶτε'	2ppma----i	'ἀγαπάω',V-
'ἀγαπᾶτε'	2ppsa----i	'ἀγαπάω',V-
\end{lstlisting}

which is transformed in:

\begin{lstlisting}[firstnumber=1, escapeinside='']
'ἀγαπᾶτε'	2ppia----i 'ἀγαπάω',V-	2ppma----i 'ἀγαπάω',V-	2ppsa----i 'ἀγαπάω',V-
\end{lstlisting}

\begin{table}
  \begin{tabular}{|l|l|l|}
        \hline
        \textbf{field}            & \textbf{category} & \textbf{possible values} \\ \thickhline
            first field  & lemma type & \specialcell{a - adjective\\c - conjunction\\d - adverb\\e - exclamation\\g - particle\\l - article\\m - numerals\\n - noun\\p - pronoun\\r - preposition\\t - participle\\v - verb\\x - miscellanea} \\ \hline
            second field & person & \specialcell{1 - first person\\2 - second person\\3 - third person} \\ \hline
            third field  & number & \specialcell{d - dual\\p - plural\\s - singular} \\ \hline
            fourth field & tense & \specialcell{g - gerund \\ p - participle \\ a - aorist\\f - future\\i - imperfect\\l - pluperfect\\p - present\\r - perfect\\t - future perfect} \\ \hline
             fifth field & mood & \specialcell{i - indicative\\m - imperative\\n - infinitive\\o - optative\\s - subjunctive} \\ \hline
             sixth field & diathesis & \specialcell{a - active\\e - energetic\\m - medial\\p - passive} \\ \hline
           seventh field & gender & \specialcell{f - feminine\\m - masculine\\n - neuter\\ o, p and q - unclear} \\ \hline
            eighth field & case & \specialcell{a - accusative\\d - dative\\g - genitive\\n - nominative\\v - vocative} \\ \hline
             ninth field & degree of comparison & \specialcell{c - comparative\\s - superlative} \\ \hline
        \hline
    \end{tabular}
\caption{The Perseus ennealiteral morphological abbreviation system.} \label{table:perseusmorph}
\end{table}

\begin{table}
  \begin{tabular}{|l|l|l|}
        \hline
        \textbf{field}            & \textbf{category} & \textbf{possible values} \\ \thickhline
            first field & person & \specialcell{1 - first person\\2 - second person\\3 - third person} \\ \hline
            second field  & number & \specialcell{d - dual\\p - plural\\s - singular} \\ \hline
            third field & tense & \specialcell{a - aorist\\f - future\\i - imperfect\\l - pluperfect\\p - present\\r - perfect\\t - future perfect} \\ \hline
             fourth field & mood & \specialcell{i - indicative\\m - imperative\\n - infinitive\\o - optative\\s - subjunctive} \\ \hline
             fifth field & diathesis & \specialcell{a - active\\e - energetic\\m - medial\\p - passive} \\ \hline
           sixth field & gender & \specialcell{f - feminine\\m - masculine\\n - neuter} \\ \hline
            seventh field & case & \specialcell{a - accusative\\d - dative\\g - genitive\\n - nominative\\v - vocative} \\ \hline
             eighth field & degree of comparison & \specialcell{c - comparative\\s - superlative} \\ \hline
             ninth field & placeholder column & \specialcell{-} \\ \hline
             tenth field & inflectibility & \specialcell{i - inflected\\n - not inflected} \\ \hline
        \hline
    \end{tabular}
\caption{The PROIEL decaliteral morphological abbreviation system.} \label{table:proielmorph}
\end{table}

\begin{table}
  \begin{tabular}{|l|l|l|}
        \hline
        \textbf{field}            & \textbf{value} & \textbf{Perseus first field equivalent} \\ \thickhline
                                A- & adjective & $\rightarrow$ a \\ \hline
                                C- & paratactic conjunctions & $\rightarrow$ c \\ \hline
                                Df & adverbs & $\rightarrow$ d \\
                                Dq & adverbial response particles (where, how, etc.) & $\rightarrow$ g \\
                                Du & adverbial question particles (where, how, etc.)  & $\rightarrow$ g \\ \hline
                                F- & Hebrew loan words & $\rightarrow$ x \\ \hline
                                G- & hypotactic conjunctions & $\rightarrow$ c \\ \hline
                                I- & illocutive particles & $\rightarrow$ g \\ \hline
                                Ma & cardinal numerals & $\rightarrow$ m \\
                                Mo & ordinal numerals & $\rightarrow$ m \\ \hline
                                Nb & nouns (in general) & $\rightarrow$ n \\
                                Ne & nouns (proper names) & $\rightarrow$ n \\ \hline
                                Pc & pronouns (reciprocative) & $\rightarrow$ p \\
                                Pd & pronouns (demonstrative) & $\rightarrow$ p \\
                                Pi & pronouns (interrogative) & $\rightarrow$ p \\
                                Pk & pronouns (reflexive) & $\rightarrow$ p \\
                                Pp & pronouns (personal) & $\rightarrow$ p \\
                                Pr & pronouns (relative) & $\rightarrow$ p \\
                                Ps & pronouns (possessive) & $\rightarrow$ p \\
                                Px & pronouns (quantitative, i.e.\ some, all, none, same, other) & $\rightarrow$ x \\ \hline
                                R- & prepositions & $\rightarrow$ r \\ \hline
                                S- & article & $\rightarrow$ l \\ \hline
                                V- & verb & $\rightarrow$ v \\ \hline
        \hline
    \end{tabular}
\caption{The PROIEL biliteral lemmatic abbreviation system.} \label{table:proiellemmata}
\end{table}

\subsubsection{Dependency data} % (fold)
\label{ssub:depdata}

% subsubsection Dependency data (end)

\subsection{Accuracy testing} % (fold)
\label{sub:accuracy-testing}

% subsection Accuracy testing (end)

% section Technical (end)

