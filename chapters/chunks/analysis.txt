LINGUISTIC DIVERSITY

The diversity of the corpus we want to treat is very high since it is
composed of texts which span more than a millennium. We need to take
this into account since this could lead to ambiguity for certain word
forms and distorted probabilities for some word forms and constructs
which have evolved in time. This is best remedied by using as diverse
a corpus as possible to train our model.

---

RELATIVE LACK OF ANCIENT GREEK TREEBANKS

An important obstacle to the development of natural language
processing tools for ancient Greek is the relative scarcity of
training material: most resources do provide morphological analysis,
but there are very few projects concerned with treebanks or databases
of semantically annotated Greek. The Perseus project, for instance,
has developed a dependency treebank for Latin and ancient Greek. It is
an admirable effort, but limited in scope and containing mainly
poetry, which is in itself valid training data, but certainly not
sufficient training data if we want our system to be able to analyse
large amounts of prose.  What's more, the project seems to be lacking
manpower and has lost steam since its inception, the last update dates
from 2012, more than a year ago at the time of this writing. 

Another interesting treebank is that hosted by the PROIEL
\footnote{Short for 'Pragmatic Resources for Indo-European Languages'}
project, which aims to offer morphologically and syntactically
annotated multilingual corpora for comparative purposes. The project,
contrary to the Perseus treebank, seems to be alive and well at the
time of this writing. This corpus contains data which can be of much
help: large swathes of Herodotus, the New Testament, and the writings
of the Byzantine historian George Sphrantzes are fully annotated, both
morphologically and syntactically. Since the Greek of the papyri is
syntactically similar to the Greek of these texts, we are afforded a
good basis for our system.

Nevertheless, probabilistic natural language processing is by virtue
of its underlying principles hungry for ever more data in order to
achieve high performance. For English, the size of the corpora is
impressive; the British National Corpus, for instance, contains a cool
one hundred million fully tagged words; the Penn treebank, which
contains about 4.5 million tagged words and three million parsed
words, is somewhat smaller but still far above the size of anything
available for ancient Greek. Therefore, we somehow need to create a
larger foundation upon which we can construct a performant
architecture. Here, we can take our cues from the field of machine
learning, where the state-of-the-art approaches rely on massive
amounts of unlabeled data which are then submitted to analysis. The
exact approaches chosen are explained in detail in the next chapter.

---

EXPANDING LINGUISTIC RESOURCES FOR ANCIENT GREEK

An enormously interesting prospect for the future of philology is the
Open Philology project, led by dr. Gregory Crane of Perseus project
fame \footnote{or rather, φημή or fama, since dr. Crane is mostly
known, either directly or indirectly, by the classicists of this
world}. This project aims to set up a digital infrastructure for
philology which will be freely and widely accessible, both passively
and actively, as it aims to both offer textual resources and allow
collaborative editing.\footnote{In this sense, it is analogous to the
open source movement in the world of computer software, and one may
hope that it brings the same benefits: a wider audience, a larger
forum for discussion and most of all, the generation of new
knowledge.} 

These resources will span the gamut of philological knowledge.  Not
only will they encompass those staples of classical scholarship,
morphosyntactic analysis and textual commentary; they will also form a
framework driven by man and machine. Among the projected features are
various linguistic annotations and tools, such as full morphosyntactic
analysis, text alignment, detection of intertextual relationships,
named entity recognition, and more. The final goal is to present all
this in a format which is fit for computational processing in order to
allow for machines to perform tasks for which manpower is
lacking. When I started this thesis, only few resources were available
in this domain; though the current state of affairs is not much
different today, it seems that the digital classics are headed for a
bright future. It is my hope that ongoing efforts will succeed in
blowing the dust off the methods of philology and propel the field
toward the widespread adoption of computational methods, a direction
which all modern disciplines must take if they do not wish to lag
behind.

---

TOKENIZATION

Tokenization is also a tricky affair given the common usage in Greek
of crasis and similar phenomena. This issue must also be resolved using a
system which is as simple as possible. Resolving this ambiguity
requires, in many instances, the use of external resources. Take the
forms ἅνθρωπος and ἄνθρωπος, for instance. While this kind of crasis
is not omnipresent in most Greek texts, it is certainly not
inexistent. Solving this kind of overlap requires a system which is
able to recognise accents and spirituses and is able to infer the
elision of the definite pronoun ὁ as well, a task which is not easy to
generalise. The founder of the Perseus project, Gregory Crane, wrote
Morpheus in the 80s, which is a morphological analysis tool for
ancient Greek. He has not made the source code for this tool publicly
available (and will not do so in the foreseeable future) but an SQL
dump is freely available from the Perseus project's website which
contains an astounding one million word forms (REFERENCE), many of which are
one-off cases exactly like this one. We will make gratuitous use of
this tool for tokenising Greek text. A possible objection is that
ambiguity can exist between several such forms; but for tokenisation,
this is hardly relevant.

Another delicate point is punctuation: do we consider the Greek
semicolon as delimiting a sentence or not? It seems to me that the
best course of action here is to simply consider these as having a
value a bit between the modern colon and semicolon as used in English
and other modern languages. The reason for this is that semantic
relations very frequently span over this symbol; therefore we would
run the risk of achieving very low accuracy rates in this domain. On
the other hand, this is also true for full sentences delimited by a
period, but less frequently so. It would also not be computationally
reasonable to check the previous sentence each time; this would cause
massive overhead and effectively at least double the input size for
our algorithm.

Once the text is tokenised, we may proceed to the next level, that is, morphological analysis. 

---

* CURRICULUM LEARNING

A technique from machine learning we will use is curriculum learning,
a highly effective technique which involves gradual training for our
artificial Hellenist. The basic idea, as indicated by the name of the
technique, is to create a curriculum, that is, a graded sequence of
successively harder learning tasks which make use of what has
previously been learned. We might, for instance, first pick the
thousand most common or morphologically simplest words, create a
partial model based on these words and then proceed with the five
thousand most common words, etc ... The approach is reliant on Zipf's
law, which states that a word's absolute frequency in a corpus is
inversely proportional to its frequency rank. (REFERENCE)

---

SUPERVISED LEARNING
PARAMETRIC AND NON-PARAMETRIC LEARNING

---

BACKPROPAGATION & GRADIENT DESCENT

---

UNSUPERVISED LEARNING: 

CLUSTERING
THE EM ALGORITHM
BAYESIAN NETWORKS
GAUSSIAN MIXTURE MODELS
HIDDEN MARKOV MODELS

---

CUTTING-EDGE TECHNIQUES
  DEEP LEARNING
  SEMI-SUPERVISED LEARNING

---

CREATING A MODEL
  NETWORK STRUCTURE
    WINDOW APPROACH
      INPUT
      LOOKUP
      LINEAR
      HARDTANH
      LINEAR
    SENTENCE APPROACH
      INPUT
      LOOKUP
      CONVOLUTION
      MAX-OVER-TIME
      LINEAR
      HARDTANH
      LINEAR
  UNSUPERVISED
    GENETIC MODEL GENERATION
    INCREASING DICTIONARIES
  SUPERVISED
  POSSIBLE IMPROVEMENTS
    SUFFIX FEATURES
    GAZETTEERS
    CASCADING
    ENSEMBLES
    PARSING
    WORD REPRESENTATIONS
    ENGINEERING A SWEET SPOT

  

---

TAGGING TEXT
  USING THE LEARNED DATA
  THE VITERBI ALGORITHM
