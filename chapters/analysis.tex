
%************************************************
\chapter{Analysis}
\label{chp:analysis}
%\minitoc\mtcskip
%************************************************

\section{Objectives}
Our main objective is the creation of a \textbf{statistical language
model} for ancient Greek using techniques from machine learning; as an
illustration, we will then apply it to the \textbf{corpus of papyri}
as provided by papyri.info.

Such a model, at heart, serves to assign probabilities to word
sequences; however, it is key that the model is also able to apply the
knowledge of these probabilities to specific problems in the analysis
of language. The problems we want to tackle are classical
\textbf{morphological analysis} and \textbf{syntactic analysis}.

The first problem largely corresponds with what is called
\textbf{part-of-speech tagging} in the natural language processing
jargon. For any token in the sentence, given its context, we want to
model to produce a morphological analysis, which produces not only the
part-of-speech, but all concomitant information as well: voice, tense,
mood, case, gender, number, person, \ldots

The second problem corresponds with what is called \textbf{shallow
parsing} (or \textbf{chunking}). Given a sequence of words, we want to
identify the main grammatical components of this sequence.  This type
of parsing stands in contrast with \textbf{deep parsing}, which aims
to produce full syntactic analyses of entire sentences.

\section{Scope of the problem}
We summarise the major component problems of our task facing us.

A first major obstacle is the size and diversity of the corpus. The
corpus of the papyri as presented on papyri.info contains more than
4.5 million words and spans nearly a millennium, by virtue of which it
inevitably contains tens of thousands of unorthodox or corrupted word
forms.  This adds a great amount of complexity: not only must our
model recognise 'normal' word forms, it must also be able to make
inferences (albeit limited ones) about unknown forms with minimal hiccups.

\begin{itemize}
\item quantity: roughly 4.5 million words
\item quality: varying from extremely corrupted text to nearly pristine documents
\item representation: from 300BC to 800AD, representing an array of different discourse registers, but not including literary texts
\item simplicity: offered in EpiDoc XML, which is more complex than
plain text, but is relatively easily converted
\item retrievability: extensive annotation using XML makes searching and retrieving text easy
\end{itemize}

The second major obstacle is the ancient Greek language itself. Though
it has lost a great deal of morphological complexity in its evolution
towards its current state, the Greek of Hellenistic, Roman and
Byzantine times is still marginally more complex than a language like
English, which is the target language for most research in natural
language processing. Commonly used techniques in NLP are still
applicable and have been used with success on other morphologically
complex languages, but given the size of the tag set for ancient Greek
morphology and syntax, it is wise to preprocess the corpus to reduce
the amount of factors that must be held into account in the creation
of our model.

Another important obstacle to the development of natural language
processing tools for ancient Greek is the relative scarcity of
training material: most resources do provide morphological analysis,
but there are very few projects concerned with treebanks or databases
of semantically annotated Greek. The Perseus project, for instance,
has developed a dependency treebank for Latin and ancient Greek. It is
an admirable effort, but limited in scope and containing mainly
poetry, which is in itself valid training data, but certainly not
sufficient training data if we want our system to be able to analyse
large amounts of prose.  What's more, the project seems to be lacking
manpower and has lost steam since its inception, the last update dates
from 2012, more than a year ago at the time of this writing. 

Another interesting treebank is that hosted by the PROIEL
\footnote{Short for 'Pragmatic Resources for Indo-European Languages'}
project, which aims to offer morphologically and syntactically
annotated multilingual corpora for comparative purposes. The project,
contrary to the Perseus treebank, seems to be alive and well at the
time of this writing. This corpus contains data which can be of much
help: large swathes of Herodotus, the New Testament, and the writings
of the Byzantine historian George Sphrantzes are fully annotated, both
morphologically and syntactically. Since the Greek of the papyri is
syntactically similar to the Greek of these texts, we are afforded a
good basis for our system.

Nevertheless, probabilistic natural language processing is by virtue
of its underlying principles hungry for ever more data in order to
achieve high performance. Therefore, we somehow need to create a
larger foundation upon which we can construct a performant
architecture. Here, we can take our cues from the field of machine
learning, where the state-of-the-art approaches rely on massive
amounts of unlabeled data which are then submitted to analysis. The
exact approaches chosen are explained in detail in the next chapter.

% \section{Tokenization}

% Tokenization is also a tricky affair given the common usage in Greek
% of crasis and similar phenomena. This issue must also be resolved using a
% system which is as simple as possible. Resolving this ambiguity
% requires, in many instances, the use of external resources. Take the
% forms \Greekἅνθρωπος \Latin and \Greekἄνθρωπος \Latin, for instance. While this kind of crasis
% is not omnipresent in most Greek texts, it is certainly not
% inexistent. Solving this kind of overlap requires a system which is
% able to recognise accents and spirituses and is able to infer the
% elision of the definite pronoun \Greekὁ \Latin as well, a task which is not easy to
% generalise. The founder of the Perseus project, Gregory Crane, wrote
% Morpheus in the 80s, which is a morphological analysis tool for
% ancient Greek. He has not made the source code for this tool publicly
% available (and will not do so in the foreseeable future) but an SQL
% dump is freely available from the Perseus project's website which
% contains an astounding one million word forms (REFERENCE), many of which are
% one-off cases exactly like this one. We will make gratuitous use of
% this tool for tokenising Greek text. A possible objection is that
% ambiguity can exist between several such forms; but for tokenisation,
% this is hardly relevant.

% Another delicate point is punctuation: do we consider the Greek
% semicolon as delimiting a sentence or not? It seems to me that the
% best course of action here is to simply consider these as having a
% value a bit between the modern colon and semicolon as used in English
% and other modern languages. The reason for this is that semantic
% relations very frequently span over this symbol; therefore we would
% run the risk of achieving very low accuracy rates in this domain. On
% the other hand, this is also true for full sentences delimited by a
% period, but less frequently so. It would also not be computationally
% reasonable to check the previous sentence each time; this would cause
% massive overhead and effectively at least double the input size for
% our algorithm.

% Once the text is tokenised, we may proceed to the next level, that is, morphological analysis. 