
%************************************************
\chapter{Analysis}
\label{chp:analysis} %\minitoc\mtcskip
%************************************************

In this chapter, the main objectives of this thesis are outlined more
precisely, placed in their scholarly context and given motivation. We
consider a \textbf{dual goal}: creating a statistical language model
of ancient Greek using machine learning techniques, and applying this
model to a corpus of documentary papyri.

We propose that both problems have not been tackled in an adequate way
up to now.

\section{Problem statement}
Firstly, we aim to develop a \textbf{language model for ancient
Greek}. We understand this to be a statistical model designed to
assign probability scores to word sequences; however, it is key that
the model is also able to apply the knowledge of these probabilities
to specific problems in the analysis of language. We extend this model
to cover the tasks of \textbf{morphological analysis} and
\textbf{partial syntactic parsing}.

\subsection{Morphological analysis}

\subsubsection{Definition}
The first problem largely corresponds with what is called
\textbf{part-of-speech tagging} in the natural language processing
jargon. For any token in the sentence, given its context, we want to
model to produce a morphological analysis, which produces not only the
part-of-speech \textit{stricto sensu}, but all concomitant information
as well: voice, tense, mood, case, gender, number, person, \ldots

\subsubsection{Benchmark}
The Perseus word study tool is currently capable of analysing any
Greek word form morphologically and offers a limited degree of
disambiguation; nonetheless, no exact quantification exists of the
correctness of these disambiguations and we cannot measure our results
against the performance of this tool. The TreeTagger model developed
in \cite{dik2008,dik2009}, on the other hand, has been evaluated for
accuracy and reaches 91\% accuracy by using an annotated training set
consisting of 150K words from the Greek New Testament and 2K words
culled from Lysias.

We aim to supersede this performance by leveraging raw textual data
from the Perseus project and the \textit{Thesaurus Linguae Graecae}
using unsupervised machine learning techniques, as detailed
\textit{infra}. In our view, given much larger corpora than used by
\citeauthor{dik2008}, we can achieve a 95\% accuracy rate with minimal
manual finetuning and integration with other morphological tools.

\subsection{Partial syntactic parsing}
\subsubsection{Definition}
The second problem corresponds with what is called \textbf{partial} or
\textbf{shallow parsing}, or \textbf{chunking}. Given a sequence of
words, we want to identify the main grammatical components of this
sequence.  This type of parsing stands in contrast with \textbf{deep
parsing}, which aims to produce full syntactic analyses of entire
sentences, including \textbf{parse trees}, which give a graphical
representation of their syntactic structure.

\subsubsection{Benchmark}
Our closest competitor for this task is \cite{mambrini2012}, as 
the method employed by \cite{lee2010dependency} to attain high parsing
accuracy is unfeasible. We stipulate that \cite{mambrini2012} goes
further than we plan to by engaging in deep parsing. Chunking as we
will perform it is more restricted and does not attempt to map
head-dependency structures. Instead, we simply look at text windows
and attempt to assign a syntactic label to the central word of each
window. For the deep parsing accuracy criterium most related to this,
label accuracy, \citeauthor{mambrini2012} achieve 62.9\% accuracy on
their only prose test set. We adopt this as our minimal benchmark.

\subsection{Application to a corpus of papyri}
\subsubsection{Definition}
Once the model is constructed, we aim to apply it to a corpus of
papyri provided digitally by \texttt{papyri.info}. The object corpus
contains about 4.5 million words. Given the nature of the provided
material, the state of these texts varies from extremely corrupted to
nearly pristine. The texts are dated from 300BC to 800AD, spanning
more than a millennium; many different discourse registers are
represented, although literary texts are not included.

Linguistically speaking, this is a less than desirable state of
affairs. By virtue of the characteristics mentioned in the previous
paragraph, the corpus will contains thousands of unorthodox or
corrupted word forms. This heightens the complexity of the task at
hand, and we cannot but lower our standards for accuracy if we wish to
proceed in an automated way. Nevertheless, we aim to provide
limited inferences for this type of token.

\subsubsection{Benchmark}
We can perform only very limited testing for this task, since there is
no extant annotation for the object corpus. Instead, we evaluate the
accuracy of our model when applied in an unsupervised fashion to a
test set selected from the papyri. To supplement this, we plan to
distribute this corpus in order to allow for manual verification by
the philological community. Our end goal is to create an annotated
version of the corpus in XML format, encoded in Unicode and following
the TEI standard for XML documents.


% The second major obstacle is the ancient Greek language itself. Though
% it has lost a great deal of morphological complexity in its evolution
% towards its current state, the Greek of Hellenistic, Roman and
% Byzantine times is still marginally more complex than a language like
% English, which is the target language for most research in natural
% language processing. Commonly used techniques in NLP are still
% applicable and have been used with success on other morphologically
% complex languages, but given the size of the tag set for ancient Greek
% morphology and syntax, it is wise to preprocess the corpus to reduce
% the amount of factors that must be held into account in the creation
% of our model.


% Another interesting treebank is that hosted by the PROIEL
% \footnote{Short for 'Pragmatic Resources for Indo-European Languages'}
% project, which aims to offer morphologically and syntactically
% annotated multilingual corpora for comparative purposes. The project,
% contrary to the Perseus treebank, seems to be alive and well at the
% time of this writing. This corpus contains data which can be of much
% help: large swathes of Herodotus, the New Testament, and the writings
% of the Byzantine historian George Sphrantzes are fully annotated, both
% morphologically and syntactically. Since the Greek of the papyri is
% syntactically similar to the Greek of these texts, we are afforded a
% good basis for our system.

% Nevertheless, probabilistic natural language processing is by virtue
% of its underlying principles hungry for ever more data in order to
% achieve high performance. Therefore, we somehow need to create a
% larger foundation upon which we can construct a performant
% architecture. Here, we can take our cues from the field of machine
% learning, where the state-of-the-art approaches rely on massive
% amounts of unlabeled data which are then submitted to analysis. The
% exact approaches chosen are explained in detail in the next chapter.

\section{Research context}

\subsection{State of the question}

While we can see from \ref{sec:relatedwork} that natural language
processing for ancient Greek seems to have garnered some attention, we
can make several observations of note on the current state of research.

The problem of Greek morphological analysis can essentially be
considered solved for individual words, as demonstrated by the Perseus
word study tool; contextual disambiguation, on the other hand, has
only been attempted once with some success in
\cite{dik2008,dik2009}. The idea of designing a system to
automatically process ancient Greek as envisioned in this work was
originally inspired by this approach.

Extending this methodology to syntactic analysis is a
\textit{desideratum}. There are only two projects concerned with
treebanks or databases of semantically annotated Greek. The Perseus
project has developed a dependency treebank for Latin and ancient
Greek [decribed in \cite{bammancrane2011}, available at
\url{http://nlp.perseus.tufts.edu/syntax/treebank/}]. It is an
admirable effort, but limited in scope and containing mainly poetry.
The project seems to be lacking manpower and has lost steam since its
inception, as the last update dates from 2012, more than a year ago at
the time of this writing.

The PROIEL project \citep{proiel}

An early prototype of this thesis attempted to use similar supervised
methods to annotate the corpus of the papyri. Despite high
expectations, experience showed that the lack of extensive annotated
corpora is a severe hindrance, as the main way to improve the accuracy
of any NLP system is to offer it more training data. Feeding 400.000
words as training data to the Stanford POS Tagger resulted in a measly
60\% accuracy on a validation set held out from the training corpus.

It is key to observe that we have touched a sore point with this
result: there is a deadlock between the development of extensive
annotated corpora and the development of natural language processing
tools for ancient Greek. The absence of sufficiently large and diverse
annotated corpora is a severe impediment to the development of such
tools; at the same time, these tools, due to lack of philological
manpower, are necessary for the extension and improvement of such
corpora! Breaking this deadlock is quintessential for further progress.

\subsection{Proposed solution}

We propose to do so by adopting alternative methodologies. Recent
literature in the field of machine learning methods for English
natural language processing revealed that state-of-the-art results
can be attained using a combination of unsupervised and supervised
learning techniques, dubbed semi-supervised approaches. Unsupervised
approaches can make use of unannotated data as a preparation for
supervised training, and work by trying to embed words in a vector
space relative to other words according to their syntactic properties.

Notably, \citeauthor{collobert2008} developed a versatile architecture
which achieved high accuracy on several NLP tasks and required a
relatively low amount of optimisation
\citep{collobert2008,collobert-2011}. The architecture was originally
applied to a diverse array of NLP tasks for English. Accuracy rates
for POS tagging reached up to 97.20\%, while for chunking, scores of
up to 93.63\% were achieved; state-of-the-art results were also
achieved for named entity recognition and semantic role labeling,
which we will not considere here. This is an impressive performance:
most of the architecture is actually shared among all tasks and the
majority of the parameters of the system are inferred through
unsupervised methods.

Given that far larger amounts of raw textual material are available
for ancient Greek, it seems that this kind of technique is suited to
the problem at hand. The 400.000 word training corpus used in the
experiment with the Stanford POS Tagger is much smaller and limited
than corpora like that offered by the Perseus project (about 7M words)
and the TLG (about 109M words at last count, though these are not
freely available). Making use of this untapped resource is
desirable. 

\hyperref[chp:design]{Chapter \ref*{chp:design}} is dedicated to an
overview of the architecture; the approach followed in
\citet{collobert-2011} and \citet{turian2010word} is respected with
amendments and simplifications where needed in order to accommodate
for some characteristics of Greek (in particular the very high
complexity of its morphology requires a subtler approach). The exact
implementation of the system is left for
\hyperref[chp:implementation]{chapter \ref*{chp:implementation}}.

\section{Possibilities}

\subsection{Corpus-based grammars and lexica}
\label{sec:corpusbasedgrammars}
Expanding our method to other texts might bring the benefit of
comprehensive corpus-based grammars and lexica, which can integrate
available data on the fly and create a self-updating and reliable web
of grammatical knowledge. Instead of focusing mainly upon a few choice
authors or laboriously trudging through the huge wealth of ancient
Greek literature to linearly create lexica and grammars, all of it
could be harnessed at once in a quantitatively precise and easily
visualisable way.

\url{http://www.digitalhumanities.org/dhq/vol/003/1/000033/000033.html}
\cite{bamman2008building,bammandhq2009}

\subsection{Historical and variational linguistics} % (fold)
\label{sec:histlinguistics}
The language of the papyri has an important role to play in the
historical linguistics of Greek; once a full annotation has been
achieved, it could be possible to implement the same methods used for
synchronic language processing to map language changes in a
statistical way; it could be possible to estimate the transition
probabilities for diachronic grammatical evolutions, which has the
potential to create a picture of the evolution of Greek that would be
both comprehensive and precise. It even has potential on a comparative
level; given the long history and meandering evolutionary trajectory
of the Greek language, one could observe from the data catalysts for
language evolution in one direction or the other and apply that
comparatively.

One might also win valuable insight into language diversity in Egypt;
using the paraliterary data already available from the Trismegistos,
linguistic phenomena and evolutions could be visualised on a map and
give insight into the diatopic, diastratic and diaphasic variation of
Egyptian koin\^e, much in the way of modern dialect survey maps but
directly linked to the original texts.

\subsection{Textual criticism}
\label{sec:textualcriticism}
Textual criticism, too, could benefit from improved access to
linguistic data; dubious \textit{passus} could be disambiguated by
comparing them to similar instances in papyri from the same period and
adapting constructions and words from them.  This technique is
harnessed by \cite{mimno2009}, who use the techniques of statistical
NLP solely for these specific critical problems.  Though textual
criticism will for the foreseeable future still necessitate trained
papyrologists, the need for a very in-depth knowledge of the corpus of
papyri can be greatly reduced by calling upon data from other parts of
the corpus to present a series of statistically possible solutions for
textual issues.