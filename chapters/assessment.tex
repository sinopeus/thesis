%************************************************
\chapter{Assessment \& conclusion}
\label{chp:assessment}
%\minitoc\mtcskip
%************************************************

\section{Evaluation \& Hypothesis}

\subsection{Unsupervised model}
\subsection[Unsupervised model]{Unsupervised model\footnote{We did not have time to compute word rankings. We did pick a few words to test the rankings, but they were not good (generally in the upper part of the spectrum for rankings).}}
For unsupervised training, we attained reasonable results as shown in the
figures in the previous section. For the first few curricula, scoring
accuracy during training steadily increased, but then decreased during
the last curriculum, using the full vocabulary. This is due to the
fact that this last curriculum contained 300K more words than the
penultimate one, all of which needed to be trained. We also integrated
sentence padding during this last curriculum, which increases the
amount of words to be processed by a large margin.

The progress of training might seem discouraging, but it is good to
keep in mind that the average score of a correct window is a good deal
higher than that of a corrupted window, and that in general the model
evolves positively until the addition of the last curriculum, which is
of relatively little importance at this stage because of the rarity of
the words it adds.

We only computed select word rankings and were confronted with the
fact that most of them were not good. We propose that the trainer
needs much more time.

\subsection{Supervised model}
The supervised model is most certainly the weakest part of our
architecture. For the sake of simplicity, we did not implement any
safeguards for our morphological analyses, and now pay for it: the
system assigns tags without too much regard for the rules of Greek
grammar. For instance, for the first sentence of our test corpus, not
a single tag is correct. Nouns are assigned tenses, moods, etc ... It
is evident that we have not even approached the accuracy rates of
\cite{mambrini2012} and \cite{dik2008,dik2009}. We believe this is in
part also due to the relatively small training corpus for the
supervised phase.


% What are the major patterns in the observations? (Refer to spatial and temporal variations.)
% What are the relationships, trends and generalizations among the results?
% What are the exceptions to these patterns or generalizations?
% What are the likely causes (mechanisms) underlying these patterns resulting predictions?
% Is there agreement or disagreement with previous work?
% Interpret results in terms of background laid out in the introduction - what is the relationship of the present results to the original question?
% What is the implication of the present results for other unanswered questions in earth sciences, ecology, environmental policy, etc....?
% Multiple hypotheses: There are usually several possible explanations for results. Be careful to consider all of these rather than simply pushing your favorite one. If you can eliminate all but one, that is great, but often that is not possible with the data in hand. In that case you should give even treatment to the remaining possibilities, and try to indicate ways in which future work may lead to their discrimination.


\section{Concluding remarks}
\label{sec:conclusion}
We have given a brief but complete \textit{status quaestionis} of the
scholarship on NLP for ancient Greek, something which we did not find
anywhere else. We consider that we have a made a modest methodological
contribution by attempting to use techniques which as of yet have not
been applied to ancient Greek, even though the results are
disappointing. Furthermore, the unsupervised technique shows promise
if given more time to train. We offer some perspectives on how our
work should be improved and continued.

\section{Further work}
\label{sec:further_work}
\subsection{Improving the language model}
\subsubsection{Larger training sets}
Improving the unsupervised model can be done in two ways: by running
the unsupervised modeling phase for more epochs, or by increasing the
amount of training data. The first option is certainly feasible at
this point, but our training set exhausts most of the available
material. 

A first option is the corpus of papyri; however, we chose to exclude
it from the training set to avoid skewing the results. But the most
obvious option is the integral \textit{Thesaurus Linguae Graecae},
which contains more than 109M words but is still not freely
available. This is an order of magnitude larger than our current
training corpus! This is a huge amount of resources which we can
exploit to improve our model.

The main reason for these texts not being published in a downloadable
format is that the TLG requires funds to run its servers. However, we
still find it hard to justify that such a rich resource for classical
scholarship should not be distributed more widely to facilitate
further study. While this is no place for an extended philippic in
favour of open-sourcing the TLG, we still think a strong case can be
presented for this.

The supervised component of our model can also be improved by running
more epochs (which we did not have time for), but especially by
increasing the size of the training corpus. We opted to only use the
PROIEL treebank and not the Perseus treebank to minimise headaches
related to the conversion of one annotation standard to
another. Especially the syntactic annotation standards used by both,
while sharing some common concepts, are very different in
implementation.

This could be fixed by developing an effective and accurate system for
this type of conversion; this has been attempted in
\cite{conf/lrec/LeeH10}, but the accuracy rate of that experiment (in
the low 80\%s for both Latin and Greek) is too low to allow extended
use. In any case, it is our opinion that developing a more unified
system for ancient Greek treebanks is a \textit{desideratum} before
the scale of current treebanks is expanded, lest we end up with two
competing standards with none of both to lead us into the fray.

\subsubsection{Linguistic foreknowledge}
We chose our training method because of its simplicity; but it is a
good question whether it is possible to use more linguistic
foreknowledge strategically. \cite{collobert-2011} equip their model
with a selective set of simple linguistic features which are tuned to
improve performance for certain tasks. For instance, they improve
performance for named entity recognition by adding a feature for
capitalisation, which is handy given the use of capital letters in
English to indicate proper nouns. They also equip their system with a
gazetteer, which is a large list of proper nouns.

Similarly, we could recruit supplemental resources to improve
performance at our chosen tasks as well as others \ref{sec:expand}. A
large database of morphological parses exists in the form of the SQL
dump of the Perseus word study tool. We could implement a supplemental
network which reduces the number of possible parses for each word
which is in this database and is trained to pick the correct one; if
it is not, we use the unmodified neural architecture to estimate a
likely parse.

A similar technique we could use is cascading. Using this technique,
we train taggers for diverse tasks which are interconnected; we apply
these taggers sequentially and use the outputs of previous taggers for
each task. In this way, we can also heavily limit possibilities. For
example, by first finding the correct part-of-speech for a word, we
can immediately eliminate certain morphological categories. A noun,
for instance, has no voice, tense or mood, and developing such
sequential tagging method would prevent us from spending valuable
computation time on these 'no-brainers'.

\subsubsection{Probabilistic parameters}
Our model as it stands is actually very simple, a card which we have
unsuccessfully tried to play out against the complexity of Greek. We
therefore propose to add extra parameters to the model. More
specifically, we would like to add supplemental parameters for
probabilistic inference on two levels: the level of morphological
parsing and the level of sentence structure. As it stands, we compute
probabilities of tags being correct for individual word windows, but
the main weakness in this approach is that we do not check either the
consistency of morphological parses or the likelihood of a given tag
sequence.

This could be remedied by developing transition tables for each
distinct problem and applying these during tagging. During training,
counts of tag transitions would be kept inside these tables and
involved in training the parameters. This is an approach also proposed
by \cite{collobert-2011} which we have not implemented due to a lack
of time. In this way, we could ensure that our tags are not as
nonsensical as they have in our experiment.

\subsubsection{Expansion and integration}
A key point for the further development of a large-scale
infrastructure for ancient Greek annotated corpora is the integration
of diverse resources. We find an example of this in the recent
merger of several papyrological resources on the web into
\texttt{papyri.info}. The recently announced Open Philology Project
\citep{crane2013} is another good example of this kind of enterprise.

We propose the development of a full NLP stack for corpus
annotation. We envision this as a library similar to the Python
Natural Language Toolkit \citep{nltkhome}, which will offer a diverse
range of tools tailored for ancient Greek: tokenizers, POS taggers,
chunkers, parsers, training tools, concordance creators ... This tool
could then be integrated with other digital classics resources.

\subsubsection{Expanding the range of tasks}
\label{sec:expand}
Another interesting prospect is the expansion of the architecture to a
larger array of tasks. The possibilities, are certainly there:
\cite{collobert-2011} explores named entity recognition
(\textit{cfr.supra}) and semantic role labeling with noteworthy
success. Deep parsing using the same type of embeddings with recurrent
neural networks has shown excellent results, as shown in
\cite{collobert2011deep}, which attains benchmark performance. 

All of this is done using large-scale unsupervised learning with a
small kernel of supervised training data, the same technique we have
applied. The same method would also fit for ancient Greek; but again
progress is blocked by a lack of annotated language data. 

% \section{Named entity recognition}
% \label{sec:ner}

% Named entity recognition is a subdiscipline in natural language
% processing which is concerned with the automatic extraction and
% localisation of all kinds of names from texts. It has been used
% extensively in literary texts with a view to discern the importance of
% certain characters throughout the text. The KU Leuven's long-standing
% Prosopographia Ptolemaica project, which aims to be a repository of
% all inhabitants of Egypt between 300 and 30 B.C., could easily benefit
% from these techniques. The abundant manual labour that has gone into
% the project could be fed as training data to and then supplemented by
% a named entity recognition engine that could also categorise personal
% names by any criteria and establish contextual relations between
% them. To take a very rudimentary example, the name 'Alexander' could
% be retrieved in all texts and a cluster of related names generated, so
% that related individuals may be placed in a web of relations; or one
% could ask, by combining the already present linguistic annotation, to
% display all adjectives which accompany the name 'Alexander'.

% It could even go further than this and also include other particular
% names, such as places, distances, monetary units, weights, and so
% on. Historians could create a comprehensive overview of, for instance,
% the inflation of Egyptian currency, or map out trade connections using
% a search for all mentions of currency, weight and places which are in
% proximity to each other.



% What are the things we now know or understand that we didn't know or understand before the present work?
% Include the evidence or line of reasoning supporting each interpretation.
% What is the significance of the present results: why should we care? 