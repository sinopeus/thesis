
%************************************************
\chapter{Implementation}
\label{chp:implementation}
%\minitoc\mtcskip
%************************************************

\section{Language and source code}
\label{sec:langsource}
\subsection{Choice of language}
\label{sec:language}
The language modeler is programmed in Python; as programming languages
go, it possesses the clearest syntax and is reasonably concise. Python
runs in an interpreter and is slower than compiled languages such as C
or Java, but this is remedied by the large amount of available
libraries designed to circumvent this issue. For computationally
demanding numerical problems such as are frequently found in machine
learning, we can use libraries such as NumPy, SciPy, Theano, ... These
offer implementations of frequently used numerical algorithms written
in C, which are compiled during the execution of the program and cached. 

In the past few years, Python has been switching from version 2 to
version 3, which brought a lot of changes in syntax and generated a
great deal of cross-compatibility problems. Despite this, many
libraries are now available for Python 3, including the ones we are
interested in using. Therefore, we chose to use Python 3.3 instead of
Python 2.7.x; it offers superior Unicode and string processing
capabilities to preceding versions.

\subsection{Source code availability}
\label{sec:sourcecode}
The source code is available at
\url{https://github.com/sinopeus/thrax}. The core structure and
numerical algorithms, encapsulated in Theano graphs, are based off
Joseph Turian's implementation of this unsupervised model builder in
Python 2. I rebuilt the entire program surrounding this numerical
component to fit my needs: the result is a documented, cleaned up and
overall improved program. I also added functionality for the
supervised phase to complete the picture. The program's configuration
system was enhanced and is now easily editable by hand as well as
programmatically. With basic knowledge of programming, it is possible
to train a network for any given NLP task by modifying the
configuration.

\subsection{Requirements}
\label{sec:requirements}

The program was written for Python 3.3.2 using development versions of
the following libraries:

\begin{itemize}
\item NumPy: 1.8.0, \url{https://github.com/numpy/numpy};
\item SciPy: 0.13.0, \url{https://github.com/scipy/scipy};
\item Theano: 0.6rc3, \url{https://github.com/Theano/Theano}.
\end{itemize}

Each of these libraries has its own dependencies, which, evidently,
the user desirous to replicate our setup should install.

\section{Implementing the network efficiently}
\label{sec:implementation}

In this section, we highlight a few optimisations applied during the
implementation of the neural network; using these, we reduce the
computational requirements of training such a network
significantly. These techniques are commonplace in machine learning
and programming in general and should not be considered original ideas
by the author.

\subsection{Lookup table}
\label{sec:ltable}
The lookup table is initialised from a dictionary file sorted by
descending order of frequency (see \textit{infra}). A hash table is
created which associates each word in the dictionary with its
frequency rank. Separately, a matrix of dimensions $D \times d^{wrd}$
is set up which is filled with random floating point numbers between
$0$ and $10^{-2}$. A lookup table operations is now a two-step process
on two discrete data structures. This allows us to store the word
sequences more compactly for processing. If necessary, we can meld
both structures into one (if we wish to redistribute the embeddings
and the dictionary together in a serialised format, for example).

\subsection{Batch processing}
\label{sec:batch}
Examples are not processed individually by the system, but in
batches. First, a batch size is chosen. Then, a number of text windows
corresponding to this batch size is read from the training
corpus. Each individual window is passed through the lookup table,
which returns matrices such as the one in \ref{eq:ltmatrix}. These
matrices are then rotated horizontally and stacked. We thus obtain a
rank-three tensor $T$ whose entries we denote as $T_{ijk}$, with $i$
corresponding to the example number within the batch, $j$
corresponding to a text window position, and $k$ corresponding to an
embedding component. Using this method, we can feed many examples in
parallel to the program. This is particularly handy if we wish to
exploit the capacities of GPUs, which are explicitly designed for
massively parallel linear algebra operations.

\subsection{Matrix-vector representation}
\label{sec:matrixvectorreps}
The representation given in the previous chapter is advantageous in
the sense that it is organised clearly, with each layer assuming
exactly one task. When actually programming such a network, it is best
to choose a data structure which reduces the amount of space needed
and simplifies the computation. We switch to matrix-vector
representation, where a network contains the following objects:

\begin{itemize}
\item an embedding matrix as defined in \ref{sec:ltable};
\item a hidden weight matrix of dimensions $n_{in} \times n_{hu}$;
\item a hidden bias vector of dimension $n_{hu}$;
\item an output weight matrix of dimension $n_{hu} \times n_{out}$;
\item a output bias vector of dimension $n_{out}$.
\end{itemize}

We generate an output by performing a simple sequence of
operations. Given the output of a lookup table operation over a text
window in matrix form, we multiply this with the hidden weight matrix
and then add the hidden bias vector to each column. The resulting
matrix is passed through the chosen nonlinear function, in our case the
hard hyperbolic tangent, which is applied to each element in the
matrix. The resulting matrix is multiplied once more, this times with
the output weight matrix, and the output bias vector is added to each
column. In this manner, we obtain a correctly sized output vector with
minimal computational overhead. An additional advantage is, due to the
properties of matrix multiplication, we can easily apply the exact
same operation to a complete batch such as described in
\ref{sec:batch} by repeating this operation over each row of the batch
tensor.

\subsection{Optimising computation with Theano}
\label{sec:theano}
Theano, proposed in \cite{bergstra+al:2010-scipy} and available at
\url{http://deeplearning.net/software/theano/}, is a numerical library
for Python which allows for efficient computations on large arrays
using symbolic expressions. We mention a few key optimisations of the
implementation made using Theano.

\subsubsection{Graphs and symbolic expressions}
\label{sec:graph}
A Theano graph is a structured representation of a computation. The
nodes of such a graph belong to one of three types: variables,
operations and applications, termed variable, op and apply nodes. The
structure and sequence of the computation is stored by connecting
these nodes; a connection is called an edge. Such graphs can be
constructed symbolically by defining a set of variables of the types
provided by Theano and then applying a sequence of operations to
them. The library registers the operations and incrementally
constructs a graph. Once the graph is constructed, we can store it as
a function which we can apply to any valid input which is of the form
of the initial variables of the graph.

For instance, we might define an input matrix as a Theano variable,
apply the necessary neural network layers to it, and then return a
function which we can apply to any variable of the same type as the
original graph input. Furthermore, Theano applies optimisations to
these functions to improve their efficiency, accuracy and execution
speed.

\subsubsection{Automatic differentiation}
\label{sec:autodiff}
This approach yields the benefit of automatic differentiation. Every
operation defined by Theano is also paired with a its derivative
function, which is precomputed. If we model a computation which
consists of the application of several operations (i.e. we create a
composite function), it is possible to apply the chain rule to the
respective derivatives of the operations of which the computation
consists; by doing this, we can find the derivative of the entire
computation. In this way, Theano can compute the derivative of
symbolic expressions of arbitrary length at minimal cost. Given the
essential role of derivative functions in machine learning in general
and in neural networks in particular, this is truly a boon. We can
forgo the arduous manual computation of the neural network layer
gradients and automate the process, and simultaneously benefit from
increased precision and efficiency.

\subsubsection{Compiling to C and CUDA}
\label{sec:graph}
Another important optimisation performed by Theano is situated 'closer
to the metal', so to speak. Python is a high-level programming
language, that is to say, it is to a great degree an abstraction of
the internal workings of a computer, hiding details such as memory
management and register operations from the user in favor of a more
intuitive way of writing programs. This reduces the amount of manual
optimisation that can be applied to a program: lower-level languages
such as C allow direct access to basic machine functions. A Python
program will typically require a running time ten to one-hundred times
longer than an equivalent C program.

For computationally demanding numerical routines which are applied
over large datasets, this is wholly undesirable. Theano can avoid
being tied down to Python-level performance by partially or fully
compiling functions constructed from graphs to C code, which is then
compiled and stored for the rest of the execution of the
program. This offers a tremendous speed increase for certain types of
computation.

Standard C programs run on the computer's central processing unit, but
the library can accelerate the computation even more by making use of
the capacities of the discrete graphical processing units which are
present in some computers. These are designed to execute very simple
instructions in a massively parallel fashion and can be programmed
using low-level languages by using application programming interfaces
(APIs) developed specifically for that purpose. 

This is termed general programming for graphical processing units, or
GPGPU; two widely used APIs exist, CUDA, which is a closed standard
developed by Nvidia for their own devices and OpenCL,an open standard
which was developed by a consortium of companies\footnote{Chief among
these is Apple Inc., which owns the trademark, as well as companies
such as AMD, IBM, Qualcomm, Intel and Nvidia.} and is not restricted
to GPUs specifically. Theano is capable of writing C programs that
interface with CUDA; compiling functions to pure C can speed the
execution time up by by one to two orders of magnitude, as mentioned
above; but optimising certain parts of the computation by interfacing
with CUDA can lead to speed increases of another order or two of
magnitude!

\section{The full process}
\label{sec:process}
\subsection{Preparing the training corpora}
\label{sec:trainingcorpora}
We begin by preprocessing the training corpora. Since we need a corpus
for each phase of learning, but most preprocessing is analogous, we
give one set of instructions and add on a few for the supervised
phase. The \texttt{prep\_scripts} directory in the repository and the
bundled CD provides Python scripts for performing all necessary steps
efficiently; the \texttt{README} file shows how to use these.

\subsubsection{General method}
\label{sec:supcorp}
Firstly, we convert all corpora to plain text. The TLG is made
available in its own format; Perseus provides all texts in XML; and
the PROIEL project offer different formats, the handiest of which will
be the CoNLL format. We strip all critical, linguistic and discourse
annotation.

We then convert the plain text corpora to Unicode characters; both the
TLG and Perseus encode all their Greek texts using ASCII by proxy of
Beta Code, but our object corpus is encoded in Unicode. We consider
the spacing and punctuation provided by the corpora as sufficient
tokenisation for our purposes. We then realign the corpora by
inserting and deleting line breaks such that each line maps to exactly
one sentence; since the approach depends on text windows centered on
one word, we add padding to both sides of the sentence by prepending
and appending the word PADDING $wsz / 2$ times before and after each
line.

After having converted all texts to this format, we merge them into
one large text file. For assessment purposes, we split the file into
nine parts training corpus and one part validation corpus. We realign
everything once more such that we obtain a file where every line maps
to exactly one word or punctuation symbol. Sentences are now delimited
using empty lines. From this file, we also create a frequency table
which we use to create a comprehensive dictionary which we use for the
lookup table layer in the network.

\subsubsection{Supervised corpora}
\label{sec:supcorp}
The creation of our supervised corpora is performed analogously, with
the difference that all annotations are exported to separate
files. The annotation systems are then normalised to one unified
system; all layout changes are performed jointly on both the text and
the annotation file to prevent misalignment. Once this is done, all
annotations are split into individual characters and distributed over
several text files for furthe processing. We want to create a one-hot
vector for each specific character; a script is provided to automate
this. The final vectors are not stored in plain text but as a
serialised matrix to reduce space usage and facilitate the
initialisation of the program.

\subsection{Training the model}
\label{sec:createmodel}

For the training hyperparameters, we chose to set embedding sizes at
50. The text window size was set at 11 due to the prevalence of
long-range dependencies in ancient Greek. The learning rates for the
neural network parameters and the embeddings were set at $1.1 \cdot
10^{-8}$ and $3.4 \cdot 10^{-11}$, respectively. The input layer size
was set equal to the window size; the output layer size was set to 1;
the hidden layer size was set to 100.

The algorithm was iterated over increasing dictionary sizes: we
started with 4.000 words and subsequently doubled the dictionary size
at each iteration. At each iteration, we validate our model; we stop
at the point of diminishing returns to avoid computational overhead,
since the full dictionary contains more than 700K word forms, and we
do not want to be calculating the pairwise ranking criterion for this
many words over our corpus, given the fact that it is already large.

The supervised phase is initialised with the embeddings created by the
unsupervised algorithm, as well as the first linear layer. The same
hyperparameters are used, except that we modify our output size
according to the task at hand by giving it the same dimensions as the
necessary output vector.

After the architecture (model and networks) is built, it is
serialised; that is to say, the internal state of the architecture
during training time is stored to disk. Serialisation allows us to
immediately load the model into memory during the execution of our
tagging program. When tagging, we use the architecture in a read-only
manner, i.e. we only predict and do not adjust parameters any more.

Tagging is essentially a process of probabilistic prediction; text
windows are passed through each of the networks, which return a
prediction of the expected features of the central words in these
windows in the form of an output vector. For tagging a sentence with
$n$ words, we create $n$ text windows and use these as input. Each
window generates an output vector; we pick the component with the
maximum score and attribute the corresponding tag to the central word
in the original text window. This process is iterated over every
sentence in the target text. 

\subsection{Preparing the object corpus}
\label{sec:createmodel}
The corpus is provided by \texttt{papyri.info} in EpiDoc XML format
and freely accessible at
\url{https://github.com/papyri/idp.data}. Each papyrus is stored in
one XML file; the format itself allows for extensive annotation,
facilitating textual searches to a great degree. However, the
techniques we have used to annotate text require it to be provided in
plain text format. We provide a script to strip all XML markup.

Once we have everything in plain text, we can process the text as we
did in the previous phase: extraneous characters are removed, one word
is placed on each line, and sentences are delimited using spaces. In
this way, we can easily align annotations with minimal headaches. We
also create an extra accompanying file equipped with line numbers for
the corresponding words in the text file in order to track each word
back to its original line.

Once the corpus is preprocessed, we can proceed to tagging. The method
described in the previous chapter is used. We tag one papyrus at a
time and then write the tagging output to disk following the directory
structure of the \texttt{papyri.info} repository. We repeat the process
for every papyrus. 
