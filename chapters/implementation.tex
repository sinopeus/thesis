
%************************************************
\chapter{Implementation and execution}
\label{chp:implementation}
%\minitoc\mtcskip
%************************************************

\section{Language and source code}
\label{sec:langsource}
\subsection{Choice of language}
\label{sec:language}

\begin{itemize}
\item Python
\item concise and simple syntax
\item slow language, fast numerical libraries: Numpy, Scipy, Theano, Matplotlib ...
\item excellent Unicode support since version 3
\end{itemize}

\subsection{Source code availability}
\label{sec:sourcecode}

\begin{itemize}
\item GitHub
\item based off J. Turian's implementation but documented, cleaned up, improved ...
\end{itemize}

\section{The full process}
\label{sec:process}

\subsection{Preparing the training corpora}
\label{sec:trainingcorpora}
\begin{itemize}
\item conversion from TLG or TEI format to raw text
\item conversion from Beta Code to Unicode
\item stripping all characters which are not relevant, such as
critical notation, paragraph markers etc.
\item lowercase all words
\item detailed tokenisation is not necessary
\item convert annotation to a unified system
\item create conversion script from classic annotation to one-hot vector annotation
\end{itemize}

\subsection{Training the model}
\label{sec:createmodel}
\begin{itemize}
\item window size of 11 words, due to the frequency of long-range dependencies in Greek
\item represent features in 50-dimensional vectors (more dimensions
  could affect the computation time adversely)
\item unsupervised iterations over increasing dictionaries: 5.000,
10.000, 20.000, 40.000, 80.000, 160.000, 360.000
\item stop at the point of diminishing returns (computing the ranking
for a dictionary of 360.000 given a corpus of size n requires 360.000
ranking formula calculations, which is bound to take a lot of time)
\item continue training, now supervised but with the same hyperparameters and parameters as the supervised model
\end{itemize}

\subsection{Preparing the object corpus}
\label{sec:createmodel}
\begin{itemize}
\item conversion from  TEI format to raw text
\item one sentence per line!
\item stripping all characters which are not relevant, such as
critical notation, paragraph markers etc. but padding the text where necessary
\item store tags sequentially
\item basic tokenisation is handled during tagging
\item run sentence through networks, then concatenate relevant outputs
\item convert from one-hot vector notation to classic annotation
\item iterate over every sentence
\item done!
\end{itemize}