
%************************************************
\chapter{Implementation}
\label{chp:implementation}
%\minitoc\mtcskip
%************************************************

\section{Language and source code}
\label{sec:langsource}
\subsection{Choice of language}
\label{sec:language}
The language modeler is programmed in Python; as programming languages
go, it possesses the clearest syntax and is reasonably concise. Python
runs in an interpreter and is slower than compiled languages such as C
or Java, but this is remedied by the large amount of available
libraries designed to circumvent this issue. For computationally
demanding numerical problems such as are frequently found in machine
learning, we can use libraries such as Numpy, Scipy, Theano, ... These
offer implementations of frequently used numerical algorithms written
in C, which are compiled during the execution of the program and cached. 

In the past few years, Python has been switching from version 2 to
version 3, which brought a lot of changes in syntax and generated a
great deal of cross-compatibility problems. Despite this, many
libraries are now available for Python 3, including the ones we are
interested in using. Therefore, we chose to use Python 3.3 instead of
Python 2.7.x; it offers superior Unicode and string processing
capabilities to preceding versions.

\subsection{Source code availability}
\label{sec:sourcecode}
The source code is available at
\url{https://github.com/sinopeus/thrax}. The core numerical
algorithms, encapsulated in Theano graphs, are based off Joseph
Turian's implementation of an unsupervised model builder in Python
2. I rebuilt the entire program surrounding this numerical component
to fit my needs: the result is a documented, cleaned up and overall
improved program. I also added functionality for the supervised phase
to complete the picture. The program's configuration system was
enhanced and is now easily editable by hand as well as
programmatically. With basic knowledge of programming, it is possible
to train a network for any given NLP task by modifying the
configuration.

\section{The full process}
\label{sec:process}

\subsection{Preparing the training corpora}
\label{sec:trainingcorpora}
\begin{itemize}
\item conversion from TLG or TEI format to raw text
\item conversion from Beta Code to Unicode
\item stripping all characters which are not relevant, such as
critical notation, paragraph markers etc.
\item lowercase all words
\item detailed tokenisation is not necessary
\item convert annotation to a unified system
\item create conversion script from classic annotation to one-hot vector annotation
\end{itemize}

\subsection{Training the model}
\label{sec:createmodel}
\begin{itemize}
\item window size of 11 words, due to the frequency of long-range dependencies in Greek
\item represent features in 50-dimensional vectors (more dimensions
  could affect the computation time adversely)
\item unsupervised iterations over increasing dictionaries: 5.000,
10.000, 20.000, 40.000, 80.000, 160.000, 360.000
\item stop at the point of diminishing returns (computing the ranking
for a dictionary of 360.000 given a corpus of size n requires 360.000
ranking formula calculations, which is bound to take a lot of time)
\item continue training, now supervised but with the same hyperparameters and parameters as the supervised model
\end{itemize}

\subsection{Preparing the object corpus}
\label{sec:createmodel}
\begin{itemize}
\item conversion from  TEI format to raw text
\item one sentence per line!
\item stripping all characters which are not relevant, such as
critical notation, paragraph markers etc. but padding the text where necessary
\item store tags sequentially
\item basic tokenisation is handled during tagging
\item run sentence through networks, then concatenate relevant outputs
\item convert from one-hot vector notation to classic annotation
\item iterate over every sentence
\item done!
\end{itemize}