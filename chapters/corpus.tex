%************************************************
\chapter{The corpus}
\label{chp:design}
\minitoc\mtcskip
%************************************************
\section{Goals}

Our objectives for the modified corpus is threefold. Firstly, we want to
morphologically annotate it with reasonable accuracy; secondly, we also want to
add syntactical annotation; thirdly, we want to ensure the corpus is compatible
with a broad range of tools.

The former two goals are the central enterprise of this thesis. Morphological
annotation will pose little theoretical problems, as the morphological features
of Greek are not subject to discussion; but for syntactical annotation, a
variety of theoretical frameworks are in existence. The fact of the matter is
that I have chosen to take the route which is technically easiest: to use the
tenets of dependency grammar. The reason is twofold: on one hand, our training
data, the New Testament annotated by PROIEL, itself is structured using a
dependency model; on the other hand, dependency trees, in contrast to
constituent analysis, assigns each word or morph one node, which is easier to
`digest' for a computer program.

Our third goal is to format the corpus in a way that will ensure broad
compatibility. This is desirable for several reasons. A first is that a broadly
used format offers many options for data treatment; for instance,
comma-separated text files can be read by many programs, from the simplest text
editor to the most complex software packages, while XML can very easily be
integrated in a web interface or parsed and transformed by a variety of
programs. A second is that corpus is to be published on GitHub for
collaborative editing, or possibly pulled into the main papyri.info repository.
This depends on our first point: the idea of GitHub is that all projects are
open source and can be contributed to by others, which is facilitated by using a
format with wide adoption.

\section{Design}
\subsection{Initial inquiries}

Before arriving at the approach described above, I attempted several other
possible methods for automatic analysis. A first idea was to integrate the XML
files in a PhiloLogic setup.  PhiloLogic is a tool developed by the ARTFL
project and the Digital Library Development center at the University of Chicago
and released under the GNU General Public License. Its original purpose was to
serve as the full-text search, retrieval and analysis tool for large databases
of French literature.

PhiloLogic has support for TEI XML and boasts an impressive
array of features: it can search through text and deliver KWIC results
filtered by frequency and metadata, as well as collocation tables and word
order information.  Furthermore, the development web page cites support for
bibliographic backends in MySQL databases, out of the box operation,
interoperability across Unix-based systems, etc.

The tool seemed promising; especially the built-in support for XML and
Unicode drew my attention, along with the built-in features for linguistic
analysis. It essentially would have made my work easier and allowed me to
invest more time in investigating a few linguistic topics. In the end, however,
the software did not fit my needs in a few respects.

Firstly, PhiloLogic requires Apache, Perl and several CPAN modules, as well as
gawk, gdbm, and agrep.  Not a huge amount of dependencies, but still less than
using Python and the NLTK, which does not require a server or SQL database to
run; also a big problem is the out-of the box incompatibility of the required
CPAn  modules with 64-bit systems. Setting PhiloLogic up was not as
easy as advertised; I had to resort to a virtual machine running 32-bit Debian
Linux until I was able to discover a method that enabled compatibility with
Mac OS X 10.7, my OS of choice.

Secondly, XML support includes standard TEI  but has issues with EpiDoc; notably,
in my experience, headers were treated as text and EpiDoc's complicated tagset
was incorrectly rendered; for instance, original readings and corrections were
concatenated in the text browser, an undesirable situation if we wish to have
any hand in our choice of corpus, and a possible source of statistical errors.
Developing brand new XSLT stylesheets to convert EpiDoc to a simpler form of
TEI markup or to raw text could perhaps have been a feasible solution; but
PhiloLogic seemed to behave erratically in general and a first attempt at
feeding the system raw text (advertised as one of its capabilities) turned out
a spathe of errors.

Thirdly, PhiloLogic's feature set is extensive, but I soon came to realize that it
does not offer much more than papyri.info already does; it is essentially a
robust concordancing application through a web interface. I wished not only to
have a lemmatised corpus searchable by word proximity, but also by word
relations, which would shed more light on the syntax of the Greek of the
papyri, as this is arguably the largest gap in scholarship due to the age of
Mayser's \emph{Grammatik} and the incomplete state of Gignac's \emph{Grammar}.
Furthermore, the tool as available to the public does not integrate the
morphological database of Greek developed for Perseus under PhiloLogic --- a
regrettable point, really, since it was exactly this capacity I wished to
exploit in the first place. Unicode search for Greek also seems to be in need
of improvement; the search form still requires transcription. Modifying the
tool would have been unfeasible given the size of the source code.

Thus I discarded PhiloLogic from my options and set out to find or develop a
simpler system.\footnote{I also considered setting up a mirror of papyri.info
on a personal server and modifying the search functionality; this turns out not
to be a task for the weak-hearted. The required setup causes substantial
overhead for our purposes, too much to be a reasonable solution.} After the
technical struggle with PhiloLogic, I decided to focus my efforts on
concordancers that could replicate its functions. A quick consultation of
Stanford's list of resources \citep{stanfordnlpres} for statistical natural
language processing and corpus-based computational linguistics directed me to
WordSmith Tools, which is regrettably Windows-only and a commercial, closed
source program to boot.  I set out on a search for open-source alternatives and
found a diversity of programs, most of them without the same functionality as
WordSmith, and invariably clunky or antiquated --- making WordSmith the only
option, but even that did not seem viable for my ends.

I continued my search and stumbled across a very interesting piece of software
that is enjoying a good amount of popularity as a didactic tool for
computational corpus linguistics; the Python NLTK, short for Natural
Language Tool Kit, is not a stand-alone application, but rather a set of ``open
source Python modules for research and development in natural language
processing and text analytics bundled with data and documentation''
\citep{nltkhome}. These modules implement diverse functionalities useful for
natural language processing and allow their easy integration into Python
programs. The Python NLTK  seemed to offer a more than solid amount of
features: corpus readers, tokenizers, stemmers, taggers, chunkers, parsers,
classifiers, clusterers, tools for semantic interpretation and metrics were all
integrated from the get-go and are easily combined and extended. The
NLTK was developed for English, but its extensibility meant it could
be applied to ancient Greek with relative ease. A further advantage is the fact
that Python is very well-suited to text manipulation and parsing XML,
which is an advantage when working with the papyri.info corpus; it is also
cross-platform: Python interpreters are available for all major platforms.

Now that I had a solid tool for interpreting corpora, the only issue remaining
was to construct a linguistically annotated corpus out of the papyri.info
corpus. The extent of the corpus makes extensive manual tagging by a single
person unfeasible; thus I set out to look for an automated tagger which could
be used, immediately or with some effort,  on ancient Greek. A variety of
taggers seem to be available, but I settled on TreeTagger, developed by Helmut
Schmid, for several reasons:

\section{Technical} % (fold)
\label{sec:corpus-technical}

\subsection{Requirements} % (fold)
\label{sub:requirements}

To create the annotated corpus as I have, it is necessary to have the following
installed (older versions may work but have not been tested):

\begin{itemize}

  \item a UNIX-like operating system, i.e. Mac OS X, any variety of Linux or
    BSD (using Windows should also be possible, since all tools used are
    portable and cross-platform, but the file location hierarchies in some
    scripts will not work);

  \item Python 3.2.3, found at \url{http://www.python.org/} (some scripts
    require Python 2.7.3: they are indicated as such at the top of the relevant
    script), with some additional libraries:

    \begin{itemize}

      \item the Python Natural Language Toolkit, found at
        \url{http://nltk.org/}, which is only compatible with Python 2.5.x to
        2.7.x (sparingly used);

      \item the numpy library, which contains a vast array of mathematical
        functionality, found at \url{http://numpy.scipy.org/}.

    \end{itemize}

  \item Perl 5.x \url{http://www.perl.org/};

  \item Saxon-HE 9.x, found at \url{http://saxon.sourceforge.net/};

  \item Helmut Schmid's TreeTagger, found at
    \url{http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/};

  \item SQLite 3, found at \url{http://www.sqlite.org/}.

\end{itemize}

All the above, excluding Mac OS X, are freely available.

% subsection Requirements (end)


\subsection{Principles of statistical natural language processing} % (fold)
\label{sub:principles-nlp}

The crux of the proposed method is the use of the aforementioned TreeTagger,
developed by Helmut Schmid, which can lemmatize and tag at the same time,
followed by another iteration using the Stanford Parser to analyse sentence
dependencies. TreeTagger can be trained for any language, and its efficiency
for highly inflected languages such as ancient Greek is due to its combination
of two tagging methods, namely n-gram tagging and binary decision tree tagging,
the combination of which makes TreeTagger the fastest part-of-speech tagger
around. The Stanford Parser is the state of the art in sentence parsing.

The following section, therefore, is intended to provide some theoretical
background on statistical natural language processing. I have based myself upon
Manning and Schütze's \textit{Foundations of Statistical Natural Language
Processing} \citep{manning1999}, the current standard reference work, as well
as \citet{koshy2004} and \citet{hopcroft2001} for concepts relating to automata
theory, as well as on \citet{bod2004} for the paragraphs on statistics.
Furthermore, to illuminate some aspects of TreeTagger's workings, I have also
integrated Schmid's articles on tagging methodology as he applied it in
TreeTagger [\citeyear{schmid1994} and \citeyear{schmid1995}]. 

\subsubsection{Context-free grammar and pushdown automata} % (fold)
\label{sub:formallang}

The basis of all statistical natural language processing is the
conceptualisation of natural language within the Chomsky hierarchy as a formal
language generated by a \textbf{stochastic context-free grammar}, that is to say, a
context-free grammar whose production rules are augmented by a probability. In
computer science, such a language is termed a context-free language and defined
as any language that can be recognized by a \textbf{nondeterministic pushdown
automaton}. We understand an nondeterministic pushdown automaton to be a
nondeterministic finite state automaton with access to an infinite stack, a
finite state automaton being an automaton which is in a state that can
transition into another state when triggered by input. Put more simply, a
nondeterministic pushdown automaton possesses the following:

\begin{itemize} 
  
  \item a \textbf{stack}, that is, a string of symbols which functions as the
    automaton's memory. The automaton only has access to the leftmost symbol in
    this string, a symbol which is dubbed the top of the stack; the top of the
    stack can be used to determine which transition the automaton will make
    next;
    
  \item an \textbf{input tape} whose symbols are scanned one by one just like a
    finite-state automaton; 
    
    \item a \textbf{finite state control unit}, which controls the state of the
      automaton. Normally this is done in response to input, but it is possible
      in some types of automata to transition into a new state without any
      input, as is the case for pushdown automata; this type of transition is
      termed $\epsilon$-transition or $\lambda$-transition.
    
\end{itemize}

A simple example of finite state automaton is a coin-operated turnstile: it has
two possible states, locked and unlocked, and two possible inputs, the
insertion of a coin or a push. Depending on the state the machine is currently
in, each of these inputs will trigger either a change or leave the machine in
its current state. For instance, if the turnstile is unlocked, a push will make
it turn and lock again; inserting a coin will do nothing.  Conversely, when it
is locked, a coin will unlock it, while a push will still leave it locked. This
type of automaton is deterministic, that is, there is only one next possible
state to transition to. A nondeterministic finite state automaton would for
example be a vending machine, which functions in a similar way but allows for
transitions to various states depending on input; different combinations of
coins and button presses will unlock different mechanisms in the machine and
let a specific item fall into the bottom compartment.

\begin{figure}
\begin{tikzpicture}[shorten >=1pt,node distance=5cm,on grid,auto] 
  \node[text width=2cm, text centered, state] (locked)   {$locked$}; 
   \node[text width=2cm, text centered, state](unlocked) [right=of locked] {$unlocked$};
   \node[text centered, node distance=2cm] (coin) [above left=of locked] {$coin$};
    \path[->] 
    (coin) edge [] node {} (locked)
    (locked) edge  [loop left] node {push} ()
          edge  [bend left=20] node {coin} (unlocked)
    (unlocked) edge  [bend left=20] node {push} (locked)
          edge [loop right] node {coin} ();
\end{tikzpicture}
\caption{Transition diagram for a deterministic finite state automaton modeling a turnstile. Adapted from \citet[762]{koshy2004}.} \label{fig:turnstile}
\end{figure}

When we add a stack to this setup, we essentially provide the automaton with an
infinite memory. Transitions will now be based upon the current state, the
input symbol and the symbol at the top of the stack. An $\epsilon$-transition
is possible as well, with $\epsilon$ replacing the input symbol. Thus, any
transition now consists of the following: the consumption of input, the
transition to a new state itself, and the replacement of the top of the stack
by any symbol (it is even possible for the current symbol to remain in place).

Natural language fits within this paradigm. Given an alphabet $\Sigma$ = \{a,
b, c, d, ...\} or even, in the case of Greek, \{$\alpha$, $\beta$, $\gamma$,
$\delta$, ...\} the initial state and top of the stack will give rise to a
sequence of characters or strings formed from the alphabet, which may be a
word, a sentence, an paragraph, ... In other words, context-free grammar provides
a simple yet precise method for mathematically describing and studying the
rules which govern the construction of natural language from smaller blocks; we
can parse generated strings (in themselves context-free languages) and by
induction assemble a grammar. We may also feed a string as input to a pushdown
automaton applying a context-free grammar; this will show whether the string is
acceptable by the rules of the grammar or not.

\begin{figure}
  \begin{center}
\begin{tikzpicture}[shorten >=1pt,node distance=3cm,on grid,auto, label distance=0.4cm] 
  \node[text centered] (input)   {$input$}; 
  \node[text centered, align=center, state, rectangle, minimum size=2cm] (fscontrol) [right=of input] {finite\\state\\control};
   \node[text centered] (acceptreject) [right=of fscontrol] {$accept/reject$};
   \node[text centered,rectangle split, rectangle split parts=3, draw, label=0:$stack$, minimum size=1.2cm] (stack) [below=of fscontrol] {$top$};
    \path[->] 
    (input) edge [] node {} (fscontrol)
    (fscontrol) edge  [] node {} (acceptreject)
                edge  [bend left=20] node {} (stack)
    (stack) edge  [bend left=20] node {} (fscontrol);
\end{tikzpicture}
\end{center}
\caption{General structure of a pushdown automaton. Adapted from \citet[220]{hopcroft2001}.} \label{fig:pushdownautomaton}
\end{figure}

\subsubsection{Stochastics and statistics} % (fold)
\label{ssub:stochastics}

As mentioned above, statistical language processing adds a stochastic element
to this model of language; not only do we analyse the possible transitions, but
we also assign probabilities to each of them. Within probabilistics, two
interpretations exist: the more widely-known one is \textbf{frequentism}, also
known as \textbf{objectivism}. It is the classical brand of statistics;
objectivism views probabilities as realities that can be measured by relative
frequencies obtained in experiments. The opposing interpretation is termed
\textbf{subjectivist} or \textbf{Bayesian} (after Thomas Bayes, discoverer of
its founding theorem), and views probabilities as degrees of belief or
uncertainty. In other words, while frequentism relies on experiment and trial,
Bayesianism relies on the observer's judgement. We are mainly interested in
Bayesian stochastics here, so let us first direct our attention to Bayes'
theorem, which is as follows in its basic form:

\begin{equation}
  \\P(H|E)=\frac{\\P(E \cap H)}{\\P(E)}.
\end{equation}

Where P(H|E) denotes the probability of a hypothesis H given evidence E.
\textbf{Conditional probability}, the probability of a given event given some
knowledge, is an important notion here. Two kinds of conditional probability
exist: \textit{a priori} and \textit{a posteriori} probability, the former
denoting the probability before considering additional information, the latter
denoting the probability after considering it. The conditional probability of
an event A given an event B which has occurred is as follows:

\begin{equation}
\\P(A|B) = \frac{\\P(A \cap B)}{\\P(B)},
\end{equation}

from which we can derive (using the fact that set intersection is symmetric,
\textit{ergo} A $\cap$ B = B $\cap$ A) that

\begin{equation}
\\P(A \cap B) = \\P(B)\\P(A|B) = \\P(A)\\P(B|A).
\end{equation}

We can now make a substituion, by which we obtain that

\begin{equation}
\\P(H|E) = \frac{\\P(E|H)\\P(H)}{\\P(E)}.
\end{equation}

A classic demonstration of Bayes' rule can be given by the solution to the
famous Monty Hall problem, a probabilistic puzzle named after the host of the
1970's American talk show \textit{Let's Make a Deal}, which I shall use as an
illustration. The problem is set up as follows.

A man is on a quiz program and given a choice between three doors to open, of
which one will reveal a car and the others nothing; the man gets to take home
whatever is behind the door he picks. The man picks a door; then the host opens
another door, always the one without a car behind it, and asks the man whether
he'd like to change doors. The question now is the following: should he switch?

Most people would not switch due to the erroneous belief that the probability
of picking any door has not changed from 0.33 for each door, or that the
probability has now become 0.5 for each remaining door. But the fact is, he
should switch! By eliminating the third door, the host has effectively joined
all probabilities beside the one for the car being behind the door the player
has originally picked, that is, switching doors at this point will double the
probability of winning the car to 0.66.

Bayes' rule is at play here. Given A, B and C, the respective probabilities of
the car being behind each door (which are all equal to 0.33), and given I, the
initial information, we now have:

\begin{equation}
 \\P(A|I) = \\P(A|I) = \\P(A|I) = \frac{1}{3}.
\end{equation}

Now we add a factor H, which indicates the opening of the third door C by the
host. By Bayes' theorem we can state the following:

\begin{equation}
  \\P(A|HI) = \frac{\\P(A|I) \\P(H|AI)}{\\P(H|I)}.
\end{equation}

Given the fact that P(A|I) = 0.33, we will focus on P(H|AI) and P(H|I), the
probabilities of respectively the host opening door C with the car at A and the
host opening door C given only I. The former is simple: if the car is behind
door A, then we must admit that the host can only open two doors, B or C, with
the probability being 0.5 for both events; \textit{ergo}, 
\begin{equation}
\\P(H|AI) = \frac{1}{2}.
\end{equation}

The remaining probability, that of the host opening door C given only info I, is the following:

\begin{equation}
\\P(H|I) = \\P(H|AI)\\P(A|I) + \\P(H|BI)\\P(B|I) + \\P(H|CI)\\P(C|I),
\end{equation}

which is due to the fact that A, B and C are mutually exclusive and exhaust all
possibilities. For P(H|BI), the probability of the host opening C when the car
is behind B and we've picked A, we know that if the car is behind door B and we
have picked door A, the host has no choice but to open C, making P(H|BI) = 1.
P(H|CI), the probability of the host opening door C when the car is behind it
and we have picked A, is obviously 0 due to the rules of the game.  Filling in
the known probabilities gives us the following:

\begin{equation}
  \\P(H|I) = \frac{1}{2} \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} = \frac{1}{2}.
\end{equation}

We now have all requisite probabilities to calculate the final probability of winning when we don't switch.

\begin{equation}
  \\P(A|HI) = \frac{\frac{1}{3} \cdot \frac{1}{2}}{\frac{1}{2}} = \frac{1}{3},
\end{equation}

which by exclusion gives us the probability of winning when switching:

\begin{equation}
  \\P(B|HI) = \frac{2}{3}.
\end{equation}

This surprising puzzle sheds some light on how Bayes' theorem works; the effect
of applying it is essentially that the order of dependence between events is
swapped. It lets us calculate the probability of P(A|HI) in terms of P(H|AI),
or to put it more informally, to use the probability of the host opening door C
when we have picked A to calculate the probability of the car being behind door
A given the host has opened door C.

Our primary interest is in its use for natural language processing. To
expound upon this, we first need to introduce the notion of Bayesian belief
networks. A good definition is given in \citet[80]{bod2004}:

\begin{quote}
Bayesian belief networks are data structures that represent probability
distributions over a collection of random variables. A network consists of a
directed acyclic graph, in which nodes represent random variables (unknown
quantities) and the edges between nodes represent causal influences between the
variables.
\end{quote}

We are particularly interested in one specific type of Bayesian belief network
-- the hidden Markov model. A Markov model is a stochastic process which has
the Markov property; that is to say, its behaviour does not depend on previous
process states, only on the current state. Markov models are classified
according to two binary axes: autonomous versus controlled and fully versus
partially observable. The hidden Markov model is autonomous and partially
observable; this makes it an excellent model for analysing natural language, as
it shares both these properties. A hidden Markov model is in itself a
stochastic finite state automaton, with each state generating an observation
and the states themselves being hidden (an important fact; it is here where
Bayes' rule comes into play). Using our observations, we can make probabilistic
inferences about the next transition that will happen in the process, thus
allowing us up to build up a collection of parameters for state transitions
that can be used to predict our next observation.

An additional important factor is that of the order of the model; the order of
the model denotes the items the model has in memory. For instance, a
first-order model will contain exactly one item in its memory, a second order
two, etc\ldots A natural language expression mapped to a model of order 0 will
simply give the frequencies for each item in the expression, whereas elevating
the order allows the establishment of probabilistic relations between sets of
items, bigrams (two items) for first-order models, trigrams (three items) for
second-order models and so on. Given sufficient training material, a program
analysing such models will be able to reproduce similar language (it has to be
said, often with little regard for making sense but with perfect
grammaticality) and be able to analyse new sentences as well based upon the
behaviour of previously stored models.

Now, there is one problem in the picture, and that is that such programs
require \textbf{sufficient} training material, and if not enough material is
given, inaccuracies will quickly arise. Such a problem easily poses itself in
the case of high morphological complexity, where tokens have much smaller
frequencies than in morphologically simple languages such as English; since
ancient Greek is of considerable morphological complexity, most taggers working
with this method will give inaccurate results. On the other hand, TreeTagger
implements an extra technique, that of binary decision trees, which allow it to
store infrequent analyses in its memory and use them when appropriate.

Our other tool, the Stanford Parser, also relies upon hidden Markov models; its
function is to analyse the dependency structure of sentences using
unlexicalized probabilistic context-free grammars, the term `unlexicalized'
implying that the use of class words to label words is reduced to only the
syntactically most important word classes; i.e.\ nouns are often not
classified.  This increases its speed and still retains a high degree of
accuracy; it is also widely known, well-supported and very extensible thanks to
the fact that it is written in Java, which opens its up to be connected with a
variety of other programs and libraries for various purposes, e.g.\ data
visualisation tools, statistical libraries, most other programming languages,
\ldots

I will now give an overview of how I extracted the training data.

\subsection{Extracting training data} % (fold)
\label{sub:extract_training_data}
To train TreeTagger and the Stanford Parser, we need annotated data that can be analysed by the programs. We have chosen the following dataset:

\begin{enumerate}
  \item For TreeTagger (lemmatisation and morphological tagging):
\begin{itemize}
  \item as a training file: the Greek New Testament and Herodotus' histories, annotated by the PROIEL project, found at \url{http://foni.uio.no:3000/};
  \item as a lexicon: the Perseus project's million-word morphological database (found at \url{http://www.perseus.tufts.edu/hopper/opensource/download} plus all parses from the training file.
\end{itemize}

\item For the Stanford Parser (dependency parsing):
\begin{itemize}
  \item as a training file: the Greek New Testament and Herodotus' histories, annotated by the PROIEL project, supplemented with the Perseus treebanks found at \url{http://nlp.perseus.tufts.edu/syntax/treebank/}.
\end{itemize}
% subsection Extracting training data (end)
\end{enumerate}

There are a great many obstacles to unifying and formatting these different
resources; for instance, the PROIEL project and the Perseus project use
different formats; the former has published its data in three different XML
formats, while the latter has made its databases available in SQL dump format.
The PROIEL project has all its Greek text stored in Unicode, while the entire
Perseus framework still relies on beta code, which created a few obstacles
along the road. Furthermore, the annotation schemes for both databases are
different; PROIEL uses ten-characters morphological abbreviations derived from
the Perseus system, with the difference that the Perseus system only use nine
characters and orders them slightly differently. Most of the conversion work
has been automatised by a series of Python scripts, but here and there some manual input is still required. 

A first step is the formatting of the PROIEL morphological database and treebank; this is done using XSL transformations which output the text 

\subsection{Accuracy testing} % (fold)
\label{sub:accuracy-testing}

% subsection Accuracy testing (end)

% section Technical (end)

