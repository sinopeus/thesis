%************************************************
\chapter{The corpus}
\label{chp:design}
\minitoc\mtcskip
%************************************************
\section{Goals}

Our objectives for the modified corpus is threefold. Firstly, we want to
morphologically annotate it with reasonable accuracy; secondly, we also want to
add syntactical annotation; thirdly, we want to ensure the corpus is compatible
with a broad range of tools.

The former two goals are the central enterprise of this thesis. Morphological
annotation will pose little theoretical problems, as the morphological features
of Greek are not subject to discussion; but for syntactical annotation, a
variety of theoretical frameworks are in existence. The fact of the matter is
that I have chosen to take the route which is technically easiest: to use the
tenets of dependency grammar. The reason Leuven : K.U.Leuven. Faculteit Letteren, 2011 is twofold: on one hand, our training
data, the New Testament annotated by PROIEL, itself is structured using a
dependency model; on the other hand, dependency trees, in contrast to
constituent analysis, assigns each word or morph one node, which is easier to
`digest' for a computer program.

Our third goal is to format the corpus in a way that will ensure broad
compatibility. This is desirable for several reasons. A first is that a broadly
used format offers many options for data treatment; for instance,
comma-separated text files can be read by many programs, from the simplest text
editor to the most complex software packages, while XML can very easily be
integrated in a web interface or parsed and transformed by a variety of
programs. A second is that corpus is to be published on GitHub for
collaborative editing, or possibly pulled into the main papyri.info repository.
This depends on our first point: the idea of GitHub is that all projects are
open source and can be contributed to by others, which is facilitated by using a
format with wide adoption.

\section{Design}
\subsection{Initial inquiries}

Before arriving at the approach described above, I attempted several other
possible methods for automatic analysis. A first idea was to integrate the XML
files in a PhiloLogic setup.  PhiloLogic is a tool developed by the ARTFL
project and the Digital Library Development center at the University of Chicago
and released under the GNU General Public License. Its original purpose was to
serve as the full-text search, retrieval and analysis tool for large databases
of French literature.

PhiloLogic has support for TEI XML and boasts an impressive
array of features: it can search through text and deliver KWIC results
filtered by frequency and metadata, as well as collocation tables and word
order information.  Furthermore, the development web page cites support for
bibliographic backends in MySQL databases, out of the box operation,
interoperability across Unix-based systems, etc.

The tool seemed promising; especially the built-in support for XML and
Unicode drew my attention, along with the built-in features for linguistic
analysis. It essentially would have made my work easier and allowed me to
invest more time in investigating a few linguistic topics. In the end, however,
the software did not fit my needs in a few respects.

Firstly, PhiloLogic requires Apache, Perl and several CPAN modules, as well as
gawk, gdbm, and agrep.  Not a huge amount of dependencies, but still less than
using Python and the NLTK, which does not require a server or SQL database to
run; also a big problem is the out-of the box incompatibility of the required
CPAn  modules with 64-bit systems. Setting PhiloLogic up was not as
easy as advertised; I had to resort to a virtual machine running 32-bit Debian
Linux until I was able to discover a method that enabled compatibility with
Mac OS X 10.7, my OS of choice.

Secondly, XML support includes standard TEI  but has issues with EpiDoc; notably,
in my experience, headers were treated as text and EpiDoc's complicated tagset
was incorrectly rendered; for instance, original readings and corrections were
concatenated in the text browser, an undesirable situation if we wish to have
any hand in our choice of corpus, and a possible source of statistical errors.
Developing brand new XSLT stylesheets to convert EpiDoc to a simpler form of
TEI markup or to raw text could perhaps have been a feasible solution; but
PhiloLogic seemed to behave erratically in general and a first attempt at
feeding the system raw text (advertised as one of its capabilities) turned out
a spathe of errors.

Thirdly, PhiloLogic's feature set is extensive, but I soon came to realize that it
does not offer much more than papyri.info already does; it is essentially a
robust concordancing application through a web interface. I wished not only to
have a lemmatised corpus searchable by word proximity, but also by word
relations, which would shed more light on the syntax of the Greek of the
papyri, as this is arguably the largest gap in scholarship due to the age of
Mayser's \emph{Grammatik} and the incomplete state of Gignac's \emph{Grammar}.
Furthermore, the tool as available to the public does not integrate the
morphological database of Greek developed for Perseus under PhiloLogic --- a
regrettable point, really, since it was exactly this capacity I wished to
exploit in the first place. Unicode search for Greek also seems to be in need
of improvement; the search form still requires transcription. Modifying the
tool would have been unfeasible given the size of the source code.

Thus I discarded PhiloLogic from my options and set out to find or develop a
simpler system.\footnote{I also considered setting up a mirror of papyri.info
on a personal server and modifying the search functionality; this turns out not
to be a task for the weak-hearted. The required setup causes substantial
overhead for our purposes, too much to be a reasonable solution.} After the
technical struggle with PhiloLogic, I decided to focus my efforts on
concordancers that could replicate its functions. A quick consultation of
Stanford's list of resources \citep{stanfordnlpres} for statistical natural
language processing and corpus-based computational linguistics directed me to
WordSmith Tools, which is regrettably Windows-only and a commercial, closed
source program to boot.  I set out on a search for open-source alternatives and
found a diversity of programs, most of them without the same functionality as
WordSmith, and invariably clunky or antiquated --- making WordSmith the only
option, but even that did not seem viable for my ends.

I continued my search and stumbled across a very interesting piece of software
that is enjoying a good amount of popularity as a didactic tool for
computational corpus linguistics; the Python NLTK, short for Natural
Language Tool Kit, is not a stand-alone application, but rather a set of ``open
source Python modules for research and development in natural language
processing and text analytics bundled with data and documentation''
\citep{nltkhome}. These modules implement diverse functionalities useful for
natural language processing and allow their easy integration into Python
programs. The Python NLTK  seemed to offer a more than solid amount of
features: corpus readers, tokenizers, stemmers, taggers, chunkers, parsers,
classifiers, clusterers, tools for semantic interpretation and metrics were all
integrated from the get-go and are easily combined and extended. The
NLTK was developed for English, but its extensibility meant it could
be applied to ancient Greek with relative ease. A further advantage is the fact
that Python is very well-suited to text manipulation and parsing XML,
which is an advantage when working with the papyri.info corpus; it is also
cross-platform: Python interpreters are available for all major platforms.

Now that I had a solid tool for interpreting corpora, the only issue remaining
was to construct a linguistically annotated corpus out of the papyri.info
corpus. The extent of the corpus makes extensive manual tagging by a single
person unfeasible; thus I set out to look for an automated tagger which could
be used, immediately or with some effort,  on ancient Greek. A variety of
taggers seem to be available, but I settled on TreeTagger, developed by Helmut
Schmid, for several reasons:

\section{Technical} % (fold)
\label{sec:corpus-technical}

\subsection{Requirements} % (fold)
\label{sub:requirements}

To create the annotated corpus as I have, it is necessary to have the following
installed (older versions may work but have not been tested):

\begin{itemize}

  \item a UNIX-like operating system, i.e. Mac OS X, any variety of Linux or
    BSD (using Windows should also be possible, since all tools used are
    portable and cross-platform, but the file location hierarchies in some
    scripts will not work);

  \item Python 3.2.3, found at \url{http://www.python.org/} (some scripts
    require Python 2.7.3: they are indicated as such at the top of the relevant
    script), with some additional libraries:

    \begin{itemize}

      \item the Python Natural Language Toolkit, found at
        \url{http://nltk.org/}, which is only compatible with Python 2.5.x to
        2.7.x (sparingly used);

      \item the numpy library, which contains a vast array of mathematical
        functionality, found at \url{http://numpy.scipy.org/}.

    \end{itemize}

  \item Perl 5.x \url{http://www.perl.org/};

  \item Saxon-HE 9.x, found at \url{http://saxon.sourceforge.net/};

  \item Helmut Schmid's TreeTagger, found at
    \url{http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/};

  \item SQLite 3, found at \url{http://www.sqlite.org/}.

\end{itemize}

All the above, excluding Mac OS X, are freely available.

% subsection Requirements (end)


\subsection{Construction process} % (fold)
\label{sub:principles-nlp}

The crux of the proposed method is the use of the aforementioned TreeTagger,
developed by Helmut Schmid; its efficiency for highly inflected languages such
as ancient Greek is due to its combination of two tagging methods, namely
n-gram tagging and binary decision tree tagging. This last statement ought to
be clarified: the following section, therefore, is intended to provide some
theoretical background on statistical natural language processing. I have based
myself upon Manning and Schütze's \textit{Foundations of Statistical Natural
Language Processing} \citep{manning1999}, the current standard reference work,
as well as \citet{koshy2004} and \citet{hopcroft2001} for some mathematical concepts.  Furthermore, to
illuminate some aspects of TreeTagger's workings, I have also integrated
Schmid's articles on tagging methodology as he applied it in TreeTagger
[\citeyear{schmid1994} and \citeyear{schmid1995}]. 


\subsubsection{Context-free grammar and pushdown automata} % (fold)
\label{sub:formallang}

The basis of all statistical natural language processing is the
conceptualisation of natural language within the Chomsky hierarchy as a formal
language generated by a stochastic context-free grammar, that is to say, a
context-free grammar whose production rules are augmented by a probability. In
computer science, such a language is termed a context-free language and defined
as any language that can be recognized by a nondeterministic pushdown
automaton (NDPA). We understand an NDPA to be a nondeterministic finite state
automaton with access to an infinite stack, a finite state automaton being an
automaton which is in a state that can transition into another state when
triggered by input. Put more simply, a nondeterministic pushdown automaton
possesses the following:

\begin{itemize} 
  
  \item a \textbf{stack}, that is, a string of symbols which functions as the
    automaton's memory. The automaton only has access to the leftmost symbol in
    this string, a symbol which is dubbed the top of the stack; the top of the
    stack can be used to determine which transition the automaton will make
    next;
    
  \item an \textbf{input tape} whose symbols are scanned one by one just like a
    finite-state automaton; 
    
    \item a \textbf{finite state control unit}, which controls the state of the
      automaton. Normally this is done in response to input, but it is possible
      in some types of automata to transition into a new state without any
      input, as is the case for pushdown automata; this type of transition is
      termed $\epsilon$-transition or $\lambda$-transition.
    
\end{itemize}

A simple example of finite state automaton is a coin-operated turnstile: it has
two possible states, locked and unlocked, and two possible inputs, the
insertion of a coin or a push. Depending on the state the machine is currently
in, each of these inputs will trigger either a change or leave the machine in
its current state. For instance, if the turnstile is unlocked, a push will make
it turn and lock again; inserting a coin will do nothing.  Conversely, when it
is locked, a coin will unlock it, while a push will still leave it locked. This
type of automaton is deterministic, that is, there is only one next possible
state to transition to. A nondeterministic finite state automaton would for
example be a vending machine, which functions in a similar way but allows for
transitions to various states depending on input; different combinations of
coins and button presses will unlock different mechanisms in the machine and
let a specific item fall into the bottom compartment.

\begin{figure}
\begin{tikzpicture}[shorten >=1pt,node distance=5cm,on grid,auto] 
  \node[text width=2cm, text centered, state] (locked)   {$locked$}; 
   \node[text width=2cm, text centered, state](unlocked) [right=of locked] {$unlocked$};
   \node[text centered, node distance=2cm] (coin) [above left=of locked] {$coin$};
    \path[->] 
    (coin) edge [] node {} (locked)
    (locked) edge  [loop left] node {push} ()
          edge  [bend left=20] node {coin} (unlocked)
    (unlocked) edge  [bend left=20] node {push} (locked)
          edge [loop right] node {coin} ();
\end{tikzpicture}
\caption{Transition diagram for a deterministic finite state automaton modeling a turnstile. Adapted from \citet[762]{koshy2004}.} \label{fig:turnstile}
\end{figure}

When we add a stack to this setup, we essentially provide the automaton with an
infinite memory. Transitions will now be based upon the current state, the
input symbol and the symbol at the top of the stack. An $\epsilon$-transition
is possible as well, with $\epsilon$ replacing the input symbol. Thus, any
transition now consists of the following: the consumption of input, the
transition to a new state itself, and the replacement of the top of the stack
by any symbol (it is even possible for the current symbol to remain in place).

Natural language fits within this paradigm. Given an alphabet $\Sigma$ = \{a,
b, c, d, ...\} or even, in the case of Greek, \{$\alpha$, $\beta$, $\gamma$,
$\delta$, ...\} the initial state and top of the stack will give rise to a
sequence of characters or strings formed from the alphabet, which may be a
word, a sentence, an alinea, ... In other words, context-free grammar provides
a simple yet precise method for mathematically describing and studying the
rules which govern the construction of natural language from smaller blocks; we
can parse generated strings (in themselves context-free languages) and by
induction assemble a grammar. We may also feed a string as input to a pushdown
automaton applying a context-free grammar; this will show whether the string is
acceptable by the rules of the grammar or not.

\begin{figure}
  \begin{center}
\begin{tikzpicture}[shorten >=1pt,node distance=3cm,on grid,auto, label distance=0.4cm] 
  \node[text centered] (input)   {$input$}; 
  \node[text centered, align=center, state, rectangle, minimum size=2cm] (fscontrol) [right=of input] {finite\\state\\control};
   \node[text centered] (acceptreject) [right=of fscontrol] {$accept/reject$};
   \node[text centered,rectangle split, rectangle split parts=3, draw, label=0:$stack$, minimum size=1.2cm] (stack) [below=of fscontrol] {$top$};
    \path[->] 
    (input) edge [] node {} (fscontrol)
    (fscontrol) edge  [] node {} (acceptreject)
                edge  [] node {} (stack)
    (stack) edge  [] node {} (fscontrol);
\end{tikzpicture}
\end{center}
\caption{General structure of a pushdown automaton. Adapted from \citet[220]{hopcroft2001}.} \label{fig:pushdownautomaton}
\end{figure}

\subsubsection{Hidden Markov models and n-grams} % (fold)
\label{ssub:markovchains}

who is Markov?

Markov property

Markov chains and decision processes vs\. hidden and visible Markov models
  criteria of observability and autonomy

uses in linguistics


% subsubsection Markov chains (end)

\subsubsection{Bayes' theorem and decision trees} % (fold)
\label{ssub:bayestheorem}

The classic Monty Hall problem

Bayesianism vs frequentism

Bayesianism in AI and machine learning


% subsubsection Bayes' theorem (end)

\subsubsection{Zipf's law} % (fold)
\label{ssub:zipflaw}
% subsubsection Zipf's law (end)

% subsection Principles of statistical natural language processing (end)

\subsection{Extracting training data} % (fold)
\label{sub:extract_training_data}

% subsection Extracting training data (end)


\subsection{Accuracy testing} % (fold)
\label{sub:accuracy-testing}

% subsection Accuracy testing (end)

% section Technical (end)

