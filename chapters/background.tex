% ************************************************
\chapter{Background}
\label{chp:background} %\minitoc\mtcskip
% ************************************************

\section{Historical background}

\subsection{The language of the papyri}
\label{sec:lpapyri}
The papyri began to be studied linguistically not by papyrologists and
historians, but rather by Biblical scholars and grammarians interested in
their relevance in the development of koin\^{e} Greek,
particularly that of the New Testament. G. N.  Hatzidakis,
W. Cr\"onert, K. Dieterich, A. Deissmann, and A.  Thumb pioneered the
field in the late nineteenth and early twentieth century, spurring a
resurgence of scholarship on the topic;\footnote{Vide
  \cite{cronert1903,deissmann1895,deissmann1897,deissmann1929,dieterich1898,thumb1901,thumb1897}.}
an excellent overview of pre-1970s research may be found in
\cite{mandilaras1973} and \cite{gignac1976,gignac1981}.

During this period, E. Mayser began work on the earliest compendious
grammar of the papyri; it limits itself to the Ptolemaic era but
explores it at length and in great detail.  The work \citep{mayser} consists of a
part on phonology and morphology, made up of three slimmer volumes,
and a part on syntax, encompassing three larger volumes. Its
composition seems to have been exhausting: it took Mayser thirty-six
years to finish volumes I.2 through II.3, with I.1 only completed in
1970 by Hans Schmoll, at which point the entire series was given a
second edition.

When casually browsing through some of its chapters (though casual is
hardly the word one would associate with the \textit{Grammatik}) it is
remarkable to see that Mayser brings an abundance of material to the
table for each grammatical observation he makes, however small it may
be. For instance, the section on diminutives essentially consists of
pages upon pages of examples categorised by their endings.

This is its great strength as a reference work - whenever one is faced
with an unusual grammatical phenomenon in any papyrus, consulting
Mayser is bound to clarify the matter; or rather, it was, for the work
is now inevitably dated.  The volumes published during Mayser's
lifetime only include papyri up to their date of publication; only the
first tome by Schmoll includes papyri up to 1968.  It is still a
largely useful resource, but it is in urgent need of refreshment.

After Mayser set the standard for the Ptolemaic papyri, a grammar of
the post-Ptolemaic papyri was the new \textit{desideratum} in
papyrology. The work had been embarked on by Salonius, Ljungvik,
Kapsomenos, and Palmer, only to be interrupted or thwarted by
circumstance or lack of resources.  \cite{salonius1927}, for
instance, only managed to write an introduction on the sources, though
he offered valuable comments on the matter of deciding how close to
spoken language a piece of writing is. \cite{ljungvik1932} contains
select studies on some points of syntax.

It is in the 1930's that we see attempts to create a grammar of the
papyri that would be the equivalent of Mayser for the post-Ptolemaic
period.  S. Kapsomenos published a series of critical
notes \citep{kapsomenos1938,kapsomenos1957} on the
subject; though he attempted at a work on the scale of the
\textit{Grammatik}, he found the resources sorely lacking, as the
existing editions of papyrus texts could not form the basis for a
systematic grammatical study. The other was L. Palmer,
who had embarked on a similar project and had already set out a
methodology \citep{palmer1934}; the war interrupted his efforts,
and he published what he had already completed, a treatise on the
suffixes in word formation \citep{palmer1945}.

A new work of some magnitude presents itself two decades later with
B. G.  Mandilaras' \textit{The verb in the Greek non-literary papyri}
\citep{mandilaras1973}. Though it does not aim to be a grammar of
the papyri, it does offer a thorough and satisfactory treatment of the
verbal system as manifest in the papyri.  Further efforts essentially
do not appear until the publication of Gignac's grammar. It is
essentially treading in the footsteps of Mayser, only with further
methodological refinement and a more limited, though still
sufficiently exhaustive, array of examples. The author, for reasons
unknown to me, only managed to complete two of the three projected
volumes, on phonology and on morphology. The volume on syntax is thus
absent, a gap only partly filled by Mandilaras' \textit{The verb in
  the Greek non-literary papyri}.

Finally, there is the aforementioned \textit{The Language of the
Papyri} \citep{lpapyri}, which does not aim to be a work on the same
scale as previous works in the field. It is a collection of articles
on various topics, the whole of which is meant to illuminate new
avenues for future research. A particularly relevant chapter for this
thesis is the last one by Porter and O'Donnell \citep{porter2010}, who
set out to create a linguistic corpus for a selection of papyri; their
tagging approach, however, is manual, and their target corpus
limited. The authors also are the creators of
\url{http://www.opentext.org/}, a project aiming for the development
of annotated Greek corpora and tools to analyse them; sadly, no
progress seems to have been made since 2005.

\subsection[Corpus linguistics]{Corpus linguistics\footnote{The
following section is based \emph{passim} on \citet{okeeffe2010}.}}
A corpus or text corpus is a large, structured collection of texts
designed for the statistical testing of linguistic hypotheses. The
core methodological concepts of this mode of analysis may be found in
the concordance, a tool first created by biblical scholars in the
Middle Ages as an aid in exegesis. Among literary scholars, the
concordance also enjoyed use, although to a lesser degree; the
eighteenth century saw the creation of a concordance to Shakespeare.

The development of the concordance into the modern corpus was not
primarily driven by the methods of biblical and literary scholars;
rather, lexicography and pre-Chomskyan structural linguistics played a
crucial role.

Samuel Johnson created his famous comprehensive dictionary of English
by means of a manually composed corpus consisting of countless slips
of paper detailing contemporary usage. A similar method was used in
the 1880s for the Oxford English Dictionary project - a staggering
three million slips formed the basis from which the dictionary was
compiled.

1950s American structuralist linguistics was the other prong of
progress; its heralding of linguistic data as a central given in the
study of language supported by the ancient method of searching and
indexing ensures its proponents may be called the forerunners of
corpus linguistics.

Computer-generated concordances make their appearance in the late
1950s, initially relying on the clunky tools of the day - punch
cards. A notable example is the Index Thomisticus, a concordance to
the works of Thomas of Aquino created by the late Roberto Busa
S.J. which only saw completion after thirty years of hard work; the
printed version spans 56 volumes and is a testament to the diligence
and industry of its author. The 1970s brought strides forward in
technology, with the creation of computerised systems to replace
catalogue indexing cards, a change that greatly benefited bibliography
and archivistics.

It is only in the 1980s and 1990s that are marked the arrival of fully
developed corpora in the modern sense of the word; for though the
basic concepts of corpus linguistics were already widely used, they
could not be applied on a large scale without the adequate tools. The
rise of the desktop computer and the Internet as well as the seemingly
ever-rising pace of technological development ensured the
accessibility of digital tools.  The old tools - punch cards,
mainframes, tape recorders and the like - were gladly cast aside in
favour of the new data carriers.

The perpetual increase of computing power equally demonstrated the
limits of large-scale corpora; while lexicographical projects that had
as their purpose to document the greatest number of possible usages
could keep increasing the size of their corpora, the size of others
went down as they whittled the data down to a specific set of uses of
language.

The possible applications of the techniques of corpus linguistics are
diverse and numerous; for they allow for a radical enlargement in
scope while remaining empirical, and remove arduous manual labour from
the equation.  Corpus linguistics can be an end to itself; it can,
however, assert an important role in broader research.
\cite[7]{okeeffe2010} mention areas such as language teaching and
learning, discourse analysis, literary stylistics, forensic
linguistics, pragmatics, speech technology, sociolinguistics and
health communication, among others.

The term `corpus' has a slightly different usage in classical
philology: it designates a structured collection of texts, which is
not forcibly intended for the testing of linguistic
hypotheses. Instead, we have, for instance, the ancient corpus
Tibullianum, or modern-day collection, for instance the Corpus
Papyrorum Judaicarum, etc. We are primarily interested in the digital
techniques used to create linguistic corpora; so let us first take a
look at the progress of the digital classics.

\subsection{The digital classics}
Classical philology, despite its status as one of the oldest and most
conservative scientific disciplines still in existence today, has in
the last fifty years found itself at the front lines of the digital
humanities movement.  Incipient efforts in the fifties and sixties,
mainly stylometric and lexical studies and the development of
concordances, demonstrated the relevance of informatics in the
classics, an evolution that was at first met with some skepticism, but
later fully embraced.

The efforts began with the aforementioned \textit{Index Thomisticus}, the first
computer-based corpus in a classical language; but the first true
impetus was the foundation of the \textit{Thesaurus Linguae Graecae} project in
1972, a monumental project with as its goal the stocking of all Greek
texts from the Homeric epics to the fall of Constantinople. Over the
years, many functions have been added to this ever more powerful tool;
and even in the beginning stages of its development, the TLG garnered
praise.

The usefulness of the tool in its current form cannot be overstated:
not only does it contain a well-formatted and easily accessible
gigantic collection of text editions whose scope and dimensions exceed
those of nearly any university library; it also offers all of these
texts in a format that allows for lexical, morphological and proximity
searches, as well as including a full version of the Liddell \& Scott
and Lewis \& Short dictionaries. The TLG has become a staple of the
digital classics.

Despite this, the TLG is becoming more and more dated as technology
progresses.  While recent years have seen the rise of Unicode as the
standard for encoding ancient Greek, the TLG still uses beta code, a
transliteration system designed to only use the ASCII character set,
and the texts are stored using an obsolete text-streaming format from
1974, which divides the text in blocks of eight kilobytes and marks
the division between segments.

A digitised version of the Liddell-Scott-Jones lexicon has been added
to the TLG's web interface, but the texts themselves have not
undergone extensive tagging, only lemmatisation.  Searching through
the database can be done by searching for specific forms of a lemma,
or by searching for all forms of a lemma, but this is essentially the
limit of the search tool's power; it is not possible to perform a
query for all possible lemmata associated with a particular form,
i.e.\ we cannot find all forms which are, for example, an active
perfect indicative.

In the wake of the TLG, several notable projects have emerged:
Brepols' Library of Latin Texts is trying hard to be for Latin texts
what the TLG is for Greek texts; the Packard Humanities Institute has
released CD's containing a selection of classical Latin works. In more
recent times, the Perseus Project has enjoyed great popularity because
of the attractive combination of an excellent selection of classical
texts with translations, good accessibility and a set of interesting
textual tools, the entire package carrying a very interesting price
tag for the average user — it is free to use, and for the greatest
part, open source as well.

The databases we have mentioned are quite general in scope; but within
the domain of classical philology, other specialised projects
exist. Within the field of papyrology the digital revolution has taken
a firm foothold. Starting with several separate databases, the field
has experienced a tendency towards convergence and integration of the
available resources, as exemplarised by the \texttt{papyri.info} website,
maintained by Columbia University, that integrates the main
papyrological databases into a single database.

A great feature of this database is the shell in which all data is
wrapped; they are compliant with the EpiDoc standard, a subset of XML
based on the TEI standard and developed specifically for epigraphical
and papyrological texts.  One may access the database’s resources
through the Papyrological Navigator and suggest corrections and
readings through the Papyrological Editor. What’s more, all data is
freely accessible under the Creative Commons License, crowd-sourced,
regularly updated, and can be downloaded for easier searching and
tweaking.

In other words, \texttt{papyri.info} has brought the open-source
mentality from the computer world into the classics. For our purposes,
this open setup is desirable, as the database is not fit for them as
it is, but can with some effort be molded into a useful tool.

\subsection{Natural language processing}
Natural language processing (henceforth NLP) is a subdiscipline in
computer science concerned with the interaction between natural human
language and computers. Its history well and truly starts in the
fifties, with a basic concept which has played a great role in natural
language processing, and computer science in general, the Turing
test. This test, put forth by Alan Turing in his seminal paper
\textit{Computing Machinery and Intelligence} \citep{turing1950},
evaluates whether a machine is intelligent or not by placing a human
in conversation with another human and a machine; if the first human
cannot tell the other human and the machine apart, the machine passes
the test.

Machine translation systems entered development, though progress soon
stalled because of technical limitations and because of methodological
obstacles: such systems were dependent on complex rulesets written by
programmers that allowed for very little flexibility. Because of the
slow return on investments made, funding for artificial intelligence
in general and machine translation specifically was drastically
reduced throughout the late sixties and the seventies.

A resurgence followed: in the eighties, advances in computational
power permitted new statistical approaches which gradually displaced
the rule-based systems used till then. The concept of generative
linguistics as espoused by Chomsky, while still possessing as firm a
foothold as ever in traditional linguistics departments, was pruned in
favour of data-driven methodology.\footnote{This is an interesting
  controversy with the two camps being represented by Peter Norvig, a
  prominent AI and machine learning researcher, and Noam Chomsky
  himself, respectively. Though this is not the place to treat it
  extensively, we refer to \cite{katz2012}, an interview on artificial
  intelligence with Noam Chomsky, and \cite{norvig2011chomsky}, in which
  Peter Norvig provides an extensive rebuttal.}

Modern natural language processing is situated on the crossroads
between various fields: artificial intelligence, computer science,
statistics, and corpus and computational linguistics. It looks to be
an exciting field for the coming years as its techniques are under
constant improvement and ever more present in our daily lives.

Most NLP software is designed explicitly with living languages in
mind; English, being a world language and the international
\textit{lingua franca}, has enjoyed most of the attention, but other
major languages have enjoyed some attention, too. Ancient languages,
however, are neglected, presumably due to their often high complexity
and the extensive study and analysis to which they have been submitted
by skilled scholars. Yet most texts have not been integrated in
annotated corpora; and though databases such as the Perseus project
contain large swathes of morphologically and sometimes syntactically
annotated text, the process has been driven largely by manual labour;
to give an exhaustive list is not appropriate here, but another such
example which is relevant is the PROIEL project \citep{proiel}, which
is also a treebank, i.e.\ a database of syntactically annotated
sentences. It contains data for Herodotus and the New Testament.

\section{Concepts and techniques}
\label{sec:shortconcepts}
While there is, of course, no room in this thesis for an extended
course in mathematics or computer science, it is necessary to have
some background in order to understand the techniques used for the
design and implementation of the architecture. While most of this
background is basic first-year university mathematics, the average
classicist will not be fully grounded in it. That does not make this
chapter the place for it. We therefore refer the curious reader to
\hyperref[chap:conceptstechniques]{appendix A}.

\section{Related work}
\label{sec:relatedwork}
Computational approaches to classical philology have been the object
of increasing interest for the last few years. While none have chosen
to focus on the language of the Greek papyri specifically, related
areas have received attention and are relevant to the task at
hand. Annotated corpora have been created, efforts to automatically
tag Greek have been made, and some have even taken a stab at using
natural language processing techniques for textual criticism. 

\subsection{Morphological analysis}

\cite{packard1973computer} was the first attempt to create a system
for the automated morphological analysis of Greek, which he dubbed
Morph. The author did not aim at creating a theoretically well-founded
tool; instead, his aim was to assist him in the creation of a textbook
and curriculum for American university students that would enable them
to start reading the classics in the original language earlier by
providing automatically generated analyses and fitting the curriculum
to the most frequent forms found in the works selected for
reading. Despite this, it is clear that the author realises the
potential of the concept.

Most of the paper is dedicated to giving examples of how the program
would analyse a given word. The method for generating parses is
rule-based. The system is equipped with a set of morphological
roots. The algorithm, when given a word, removes suffixes until it
finds a match in this set of roots. If no matching roots are found,
the same procedure is repeated with the algorithm now stripping
prefixes from each word. The system proposed possesses several
weaknesses: accents are completely ignored (most likely due to
relatively limited technical resources at the time \footnote{A rather
quaint detail is that computing time had to be rented; the author
mentions having to use one dollar's worth of computing time to analyse
Plato's \textit{Apology of Socrates}}), and the rule-based system relies heavily on
the author's knowledge of Greek.

A critical assessment of the methodology and results of this paper is
not possible: no concrete measure of the system's accuracy is
given. The source code is also nowhere to be found; even if it was
freely available, testing it would be a complicated affair, as it is
written in assembly language specific to the IBM 360 mainframe and not
compatible with modern computer architectures. We mention it for
historical reasons, as Packard's method is central in the development
of later morphological analysis tools. 

Notably, Gregory Crane's \textit{Morpheus} was developed in C and Lisp
on top of Packard's implementation by augmenting the analyser with a
generative component. Work on the system, that would later become the
backend for the Perseus morphological analyser, began in 1984 and was
aided by two graduate students at Berkeley, Neel Smith and Joshua
Kosman.

The generative component of the system creates an extensive table of
possible word forms using stems and suffixes such as provided by
Morph. The exact system is detailed in \cite{crane1991generating}. *MORE DETAIL*

Morphological parsing is essentially done by looking up forms in this
word table. This comes at the cost of a lot of space; on the other
hand, parsing a word which is in the database is a very fast
operation, since the word itself doesn't have to be manipulated, and
the tables have been maintained and improved for years using
user input.

An important issue is that of parse ambiguity. The morphological
complexity of Greek ensures that over 50\% of words present in the
parse table have multiple parses \citep{dik2008}. Disambiguation is
therefore key for the next development stage of morphological analysis
tools for ancient Greek.

Despite this, Morpheus is without question the best tool for Greek
morphological analysis available. For years, it has been a core
component of the success of the Perseus project. Recently, there has
been a drive to create an open source version of the original
program,\footnote{\textit{Pace} \cite{blackwell2009}.} which is now
available on GitHub at \url{https://github.com/PerseusDL/morpheus}.

\cite{lee2008nearest} offers a new approach to the analysis of Greek
morphology by using machine learning methods melded with the approach
used by Morpheus. The method proposed relies on large amounts of data
and makes use of nearest-neighbour analysis techniques. This stands in
contrast to the traditional rule-driven approach used by earlier
parsers such as \cite{packard1973computer} or
\cite{crane1991generating}.

Affix transformations similar to the ones described in
\cite{packard1973computer} are applied to word forms in order to analyse their
relation to other forms. A nearest-neighbour metric is established on
the basis of the number of these transformations needed to generate
another extant word form. As training data, an annotated corpus is fed 
architecture, supported by a large amount of unlabelled data to
facilitate the prediction of verbal stems.

An accuracy of 98.2\% is achieved for words present in the training
set, with the remaining forms necessitating contextual
disambiguation. For words not present in the training set, an average
accuracy of 85.7\% is achieved with most of the loss in accuracy with
most errors due to what the author terms `novel roots', which are
stems which are not directly derivable from a word form and are
analysed with a practical 50\% accuracy (though this was improved to
65\% by loosening the standards by which accuracy was measured).

The use of machine learning techniques in this paper is laudable and
certainly not badly executed, but the general methodology and in
particular the choice of training corpora is problematic. An annotated
version of the first five books Septuagint is used to establish these
metrics. This corpus consists of 470K words and can be reduced to a
set of about 37K unique words. It is unnecessary to restrict training
to a corpus of sentences if one does not look at n-grams, but only at
stand-alone word forms.

The question, then, is: why not use the output of Morpheus, the system
designed by Gregory Crane and honed over the years, as training
material? It is freely available as an SQL database and contains
\textasciitilde1M unique parses which are generated using very large
corpora for verification. The nearest-neighbor metric may be applied
to these word forms and arguably form a better picture, as well as
allowing for better extrapolation to data not present in the training
corpus.

A last notable result \citep{dik2008,dik2009} in morphological
annotation we will only mention in passing in this section; it is
treated in more detail \textit{infra} in \ref{sec:dikwhaling}, as
it is relevant not only to the problem of morphological analysis, but
also to that of corpus annotation, our second major goal.

\subsection{Syntactic parsing}
Efforts to develop a model for syntactic parsing of ancient Greek are
in an embryonic stage. \cite{mambrini2012} contains a few experiments
with syntactically parsing ancient Greek. Training data was taken
exclusively from the Perseus ancient Greek dependency treebank and
partitioned into different sets to observe the influence of
differences in author and genre on the results. These were split up
into two datasets for training and validation, respectively. They were
then used to train MaltParser \citep{nivre2006maltparser}. Initial
performance on the Homeric texts is disappointing due to the scarcity
of resources for training the parser. 44.1\% tokens were given a fully
correct labeling, including relations and their head word, 60.3\% were
assigned a head word correctly, and 49.2\% were assigned a label
correctly.

Adjusting the hyperparameters and features of the model manually
yielded a considerable boost in performance, with accuracy rates
increasing to 71.72\%, 78.26\% and 81.62\% respective to the
aforementioned accuracy criteria on the Homeric texts. The improved
model is then tested on Hesiod, Sophocles and Plato. Unsurprisingly,
performance on the Hesiodic poems is far better than on the Sophocles
and Plato, which demonstrates that the barrier between textual genres
is a serious obstacle for this type of parsing. The authors are
currently working on expanding their training set and testing
different systems.

Another interesting approach to the problem of parsing ancient Greek
is found in \cite{lee2010dependency}. The author develops and trains a
parsing system for the Septuagint which relies on two resources: a
treebank of the New Testament, made available by PROIEL, and a
parallel text of the Septuagint in the original Hebrew. The original
is annotated with cantillation marks, which serve as prosodic markers
to be observed during public chanting of the religious texts. These
marks at times facilitate disambiguation of sentence parses for the
Hebrew text, which the author exploits to improve the Greek
parses. Remarkable results are achieved: 79.4\% of words are attached
to their correct head word. Among these, 88.5\% also receive correct
labels, leading to general accuracy rate of 70.7\%, results which are
comparable to those attained by \cite{mambrini2012}.

\subsection{Corpus annotation}
\label{sec:dikwhaling}
In two papers based off their workshops on the topic
\citep{dik2008,dik2009}, H. Dik and R. Whaling (a classics professor
and computer scientist turned classicist, respectively, both at the
University of Chicago) demonstrate a relatively simple methodology for
morphological tagging of a corpus of ancient Greek in the context of
the Perseus under PhiloLogic project under Helma Dik.  They trained
Helmut Schmid's TreeTagger (found at
\url{http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/} and
extensively described in \cite{schmid1994,schmid1995}) using a corpus
of Homeric and New Testament Greek and applied it to run over their
three-million word corpus. Initial results achieved about 80\%
accuracy and after adjustments rose up to 88\%; since the corpus was
designed for academic use, large swathes of text were then manually
disambiguated to the best of the ability of the authors and a team of
volunteers.

Their effort is remarkable in the sense that it is the only instance
of an automatically annotated corpus of ancient Greek we have managed
to find. While Perseus offers a morphological analysis tool, this tool
is designed to assist linear reading and generates parses on the fly
from a database, offering several options if several parses are
possible. Recently, this system has been improved using user votes,
frequency tables and a simple system of bigrams, in order to return
the most likely parse. Dik and Whaling's corpus, though, has been
annotated with the explicit intention of storing all parses and their
context in a relational database. This makes it possible to perform
morphological searches. For linguistic purposes, this is a very
interesting tool.
