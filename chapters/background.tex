
% ************************************************
\chapter{Background}
\label{chp:background} %\minitoc\mtcskip
% ************************************************

\section{Historical background}
\subsection{The language of the papyri}

The papyri began to be studied linguistically not by papyrologists and
historians, but rather by Bible scholars and grammarians interested in
their relevance in the development began to koin\^{e} Greek,
particularly that of the New Testament. G. N.  Hatzidakis,
W. Cr\"onert, K. Dieterich, A. Deissmann, and A.  Thumb pioneered the
field in the late nineteenth and early twentieth century, spurring a
resurgence of scholarship on the topic;\footnote{Vide
  \cite{cronert1903,deissmann1895,deissmann1897,deissmann1929,dieterich1898,thumb1901,thumb1897}.}
an excellent overview of pre-1970s research may be found in
\cite{mandilaras1973} and \cite{gignac1976,gignac1981}.


During this period, E. Mayser begon work on the earliest compendious
grammar of the papyri; it limits itself to the Ptolemaic era but
explores it at length and in great detail.  The work \citep{mayser} consists of a
part on phonology and morphology, made up of three slimmer volumes,
and a part on syntax, encompassing three larger volumes. Its
composition seems to have been exhausting: it took Mayser thirty-six
years to finish volumes I.2 through II.3, with I.1 only completed in
1970 by Hans Schmoll, at which point the entire series was given a
second edition.

When casually browsing through some of its chapters (though casual is
hardly the word one would associate with the \textit{Grammatik}) it is
remarkable to see that Mayser brings an abundance of material to the
table for each grammatical observation he makes, however small it may
be. For instance, the section on diminutives essentially consists of
pages upon pages of examples categorised by their endings.

This is its great strength as a reference work - whenever one is faced
with an unusual grammatical phenomenon in any papyrus, consulting
Mayser is bound to clarify the matter; or rather, it was, for the work
is now inevitably dated.  The volumes published during Mayser's
lifetime only include papyri up to their date of publication; only the
first tome by Schmoll includes papyri up to 1968.  It is still a
largely useful resource, but it is in urgent need of refreshment.

After Mayser set the standard for the Ptolemaic papyri, a grammar of
the post-Ptolemaic papyri was the new \textit{desideratum} in
papyrology. The work had been embarked on by Salonius, Ljungvik,
Kapsomenos, and Palmer, only to be interrupted or thwarted by
circumstance or lack of resources.  \cite{salonius1927}, for
instance, only managed to write an introduction on the sources, though
he offered valuable comments on the matter of deciding how close to
spoken language a piece of writing is. \cite{ljungvik1932} contains
select studies on some points of syntax.

It is in the 1930's that we see attempts to create a grammar of the
papyri that would be the equivalent of Mayser for the post-Ptolemaic
period.  S. Kapsomenos published a series of critical
notes \citep{kapsomenos1938,kapsomenos1957} on the
subject; though he attempted at a work on the scale of the
\textit{Grammatik}, he found the resources sorely lacking, as the
existing editions of papyrus texts could not form the basis for a
systematic grammatical study. The other was L. Palmer,
who had embarked on similar project and had already set out a
methodology \citep{palmer1934}; the war interrupted his efforts,
and he published what he had already completed, a treatise on the
suffixes in word formation \citep{palmer1945}.

A new work of some magnitude presents itself two decades later with
B. G.  Mandilaras' \textit{The verb in the Greek non-literary papyri}
\citep{mandilaras1973}. Though it does not aim to be a grammar of
the papyri, it does offer a thorough and satisfactory treatment of the
verbal system as manifest in the papyri.  Further efforts essentially
do not appear until the publication of Gignac's grammar. It is
essentially treading in the footsteps of Mayser, only with further
methodological refinement and a more limited, though still
sufficiently exhaustive, array of examples. The author, for reasons
unknown to me, only managed to complete two of the three projected
volumes, on phonology and on morphology. The volume on syntax is thus
absent, a gap only partly filled by Mandilaras' \textit{The verb in
  the Greek non-literary papyri}.

Finally, there is the aforementioned \textit{The Language of the
  Papyri} \citep{lpapyri}, which does not aim to be a work on the same
scale as the aforementioned. It is a collection of articles on various
topics, the whole of which is meant to illuminate new avenues for
future research. A particularly relevant chapter for this thesis is
the last one by Porter and O'Donnell \citep{porter2010}, who set out
to create a linguistic corpus for a selection of papyri; their tagging
approach, however, is manual, and their target corpus limited. The
authors also are the creators of \url{http://www.opentext.org/}, a
project aiming for the development of annotated Greek corpora and
tools to analyse them; sadly, no progress seems to have been made
since 2005.

\subsection{Corpus linguistics} A\footnote{The following section is
  based \emph{passim} on \citet{okeeffe2010}.} corpus or text corpus is
a large, structured collection of texts designed for the statistical
testing of linguistic hypotheses. The core methodological concepts of
this mode of analysis may be found in the concordance, a tool first
created by biblical scholars in the Middle Ages as an aid in
exegesis. Among literary scholars, the concordance also enjoyed use,
although to a lesser degree; the eighteenth century saw the creation
of a concordance to Shakespeare.

The development of the concordance into the modern corpus was not
primarily driven by the methods of biblical and literary scholars;
rather, lexicography and pre-Chomskyan structural linguistics played a
crucial role.

Samuel Johnson created his famous comprehensive dictionary of English
by means of a manually composed corpus consisting of countless slips
of paper detailing contemporary usage. A similar method was used in
the 1880s for the Oxford English Dictionary project - a staggering
three million slips formed the basis from which the dictionary was
compiled.

1950s American structuralist linguistics was the other prong of
progress; its heralding of linguistic data as a central given in the
study of language supported by the ancient method of searching and
indexing ensures its proponents may be called the forerunners of
corpus linguistics.

Computer-generated concordances make their appearance in the late
1950s, initially relying on the clunky tools of the day - punch
cards. A notable example is the Index Thomisticus, a concordance to
the works of Thomas of Aquino created by the late Roberto Busa
S.J. which only saw completion after thirty years of hard work; the
printed version spans 56 volumes and is a testament to the diligence
and industry of its author. The 1970s brought strides forward in
technology, with the creation of computerised systems to replace
catalogue indexing cards, a change that greatly benefited bibliography
and archivistics.

It is only in the 1980s and 1990s that are marked the arrival of fully
developed corpora in the modern sense of the word; for though the
basic concepts of corpus linguistics were already widely used, they
could not be applied on a large scale without the adequate tools. The
rise of the desktop computer and the Internet as well as the seemingly
ever-rising pace of technological development ensured the
accessibility of digital tools.  The old tools - punch cards,
mainframes, tape recorders and the like - were gladly cast aside in
favour of the new data carriers.

The perpetual increase of computing power equally demonstrated the
limits of large-scale corpora; while lexicographical projects that had
as their purpose to document the greatest number of possible usages
could keep increasing the size of their corpora, the size of others
went down as they whittled the data down to a specific set of uses of
language.

The possible applications of the techniques of corpus linguistics are
diverse and numerous; for they allow for a radical enlargement in
scope while remaining empirical, and remove arduous manual labour from
the equation.  Corpus linguistics can be an end to itself; it can,
however, assert an important role in broader research.
\cite[7]{okeeffe2010} mention areas such language teaching and
learning, discourse analysis, literary stylistics, forensic
linguistics, pragmatics, speech technology, sociolinguistics and
health communication, among others.

The term `corpus' has a slightly different usage in classical
philology: they designate a structured collection of texts, but that
collection is not primarily intended for the testing of linguistic
hypotheses. Instead, we have, for instance, the ancient corpus
Tibullianum, or modern-day collection, for instance the Corpus
Papyrorum Judaicarum, etc. We are primarily interested in the digital
techniques used to create linguistic corpora; so let us first take a
look at the progress of the digital classics.


\subsection{The digital classics}

Classical philology, despite its status as one of the oldest and most
conservative scientific disciplines still in existence today, has in
the last fifty years found itself at the front lines of the digital
humanities movement.  Incipient efforts in the fifties and sixties,
mainly stylometric and lexical studies and the development of
concordances, demonstrated the relevance of informatics in the
classics, an evolution that was at first met with some skepticism, but
later fully embraced.

The efforts began with the aforementioned Index Thomisticus, the first
computer-based corpus in a classical language; but the first true
impetus was the foundation of the Thesaurus Linguae Graecae project in
1972, a monumental project with as its goal the stocking of all Greek
texts from the Homeric epics to the fall of Constantinople. Over the
years, many functions have been added to this ever more powerful tool;
and even in the beginning stages of its development, the TLG garnered
praise.

The usefulness of the tool in its current form cannot be overstated:
not only does it contain a well-formatted and easily accessible
gigantic collection of text editions whose scope and dimensions exceed
those of nearly any university library; it also offers all of these
texts in a format that allows for lexical, morphological and proximity
searches, as well as including a full version of the Liddell \& Scott
and Lewis \& Short dictionaries. The TLG has become a staple of the
digital classics.

Despite this, the TLG is becoming more and more dated as technology
progresses.  While recent years have seen the rise of Unicode as the
standard for encoding ancient Greek, the TLG still uses beta code, a
transliteration system designed to only use the ASCII character set,
and the texts are stored using an obsolete text-streaming format from
1974, which divides the text in blocks of eight kilobytes and marks
the division between segments.

A digitised version of the Liddell-Scott-Jones lexicon has been added
to the TLG's web interface, but the texts themselves have not
undergone extensive tagging, only lemmatisation.  Searching through
the database can be done by searching for specific forms of a lemma,
or by searching for all forms of a lemma, but this is essentially the
limit of the search tool's power; it is not possible to perform a
query for all possible lemmata associated with a particular form,
i.e.\ we cannot find all forms which are, for example, an active
perfect indicative.

In the wake of the TLG, several notable projects have emerged:
Brepols' Library of Latin Texts is trying hard to be for Latin texts
what the TLG is for Greek texts; the Packard Humanities Institute has
released CD's containing a selection of classical Latin works. In more
recent times, the Perseus Project has enjoyed great popularity because
of the attractive combination of an excellent selection of classical
texts with translations, good accessibility and a set of interesting
textual tools, the entire package carrying a very interesting price
tag for the average user — it is free to use, and for the greatest
part, open source as well.

The databases I have mentioned are quite general in scope; but within
the domain of classical philology, other specialised projects
exist. Within the field of papyrology, for instance, the digital
revolution has taken a firm foothold. Starting with several separate
databases, the field has experienced a tendency towards convergence
and integration of the available resources, as exemplarised by the
papyri.info website, maintained by Columbia University, that
integrates the main papyrological databases into a single database.

A great feature of this database is the shell in which all data is
wrapped; they are compliant with the EpiDoc standard, a subset of XML
based on the TEI standard and developed specifically for epigraphical
and papyrological texts.  One may access the database’s resources
through the Papyrological Navigator and suggest corrections and
readings through the Papyrological Editor. What’s more, all data is
freely accessible under the Creative Commons License, crowd-sourced,
regularly updated, and can be downloaded for easier searching and
tweaking.

In other words, papyri.info has brought the open-source mentality from
the computer world into the classics. For our purposes, this open
setup is desirable, as the database is not fit for them as it is, but
can with some effort be molded into a useful tool.

\subsection{Natural language processing}

Natural language processing (henceforth NLP) is a subdiscipline in
computer science concerned with the interaction between natural human
language and computers. Its history well and truly starts in the
fifties, with a basic concept which has played a great role in natural
language processing, and computer science in general, the Turing
test. This test, put forth by Alan Turing in his seminal paper
\textit{Computing Machinery and Intelligence} \citep{turing1950},
evaluates whether a machine is intelligent or not by placing a human
in conversation with another human and a machine; if the first human
cannot tell the other human and the machine apart, the machine passes
the test.

Machine translation systems entered development, though progress soon
stalled because of technical limitations and because of methodological
obstacles: such systems were dependent on complex rulesets written by
programmers that allowed for very little flexibility. Because of the
slow return on investments made, funding for artificial intelligence
in general and machine translation specifically was drastically
reduced throughout the late sixties and the seventies.

A resurgence followed: thanks to advances in computational power and
the decline of Chomskyan linguistics in natural language processing,
which had been the dominant theoretical vantage point in the preceding
thirty years, the eighties were marked by the introduction of
statistical machine translation, which is fundamentally based on the
tenets of corpus linguistics. Modern natural language processing is
therefore situated on the crossroads between various fields:
artificial intelligence, computer science, statistics, and corpus and
computational linguistics. It looks to be an exciting field for the
coming years as its techniques are under constant improvement and ever
more present in our daily lives.

Most NLP software is designed explicitly with living languages in
mind; English, being a world language and the international
\textit{lingua franca}, has enjoyed most of the attention, but other
major languages have enjoyed some attention, too. Ancient languages,
however, are neglected, presumably due to their often high complexity
and the extensive study and analysis to which they have been submitted
by skilled scholars. Yet most texts have not been integrated in
annotated corpora; and though databases such as the Perseus project
contain large swathes of morphologically and sometimes syntactically
annotated text, the process has been driven largely by manual labour;
to give an exhaustive list is not appropriate here, but another such
example which is relevant is the PROIEL project \citep{proiel}, which
is also a treebank, i.e.\ a database of syntactically annotated
sentences. It contains data for Herodotus and the New Testament.

\section{Concepts and techniques}


While there is, of course, no room in this thesis for an extended
course in mathematics or computer science, it is necessary to have
some background in order to understand the techniques used for the
design and implementation of the architecture. What follows is an
brief overview of important concepts and techniques from mathematics,
artificial intelligence and natural language processing applied in
this thesis. For the less mathematically inclined reader, formalism
has been reduced to a minimum; instead, for each concept and
technique, only a layman's explanation is given. For further
formalism, see \vref{chap:conceptstechniques}.

\section{Related work}

Computational approaches to classical philology have been the object
of increasing interest for the last few years. While none have chosen
to focus on the language of the Greek papyri specifically, related
areas have received attention and are relevant to the task at
hand. Annotated corpora have been created, efforts to automatically
tag Greek have been made, and some have even taken a stab at using
natural language processing techniques for textual criticism. 
% \subsection{Statistical methods for textual criticism}

% \subsubsection{D. Mimno and H. Wallach}
% \cite{mimno2009}

\subsection{Artificial intelligence techniques applied to ancient Greek linguistics}

\subsubsection{H. Dik and R. Whaling}
In two papers based off their workshops on the topic
\citep{dik2008,dik2009}, H. Dik and R. Whaling (a classics professor
and computer scientist turned classicist, respectively, both at the
University of Chicago) demonstrate a relatively simple methodology for
morphological tagging of a corpus of ancient Greek in the context of
the Perseus under PhiloLogic project under Helma Dik.  They trained
Helmut Schmid's TreeTagger (found at
\url{http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/} and
extensively described in \cite{schmid1994,schmid1995}) using a corpus
of Homeric and New Testament Greek and applied it to run over their
three-million word corpus. Initial results achieved about 80\%
accuracy and rose up to 88\%; since the corpus was designed for
academic use, large swathes of text were then manually disambiguated
to the best of the ability of the authors and a team of volunteers.

Their effort is remarkable in the sense that it is the only instance
of an automatically annotated corpus of ancient Greek I have managed
to find. While Perseus offers a morphological analysis tool, this tool
is designed to assist linear reading and generates parses on the fly
from a database, offering several options if several parses are
possible. Perseus does not consider the context; Dik and Whaling's
corpus, though, has been annotated with the explicit intention of
doing so and then storing all parses and their context in a relational
database. This makes it possible to perform morphological searches,
which, for linguistic purposes, is very interesting.

\subsubsection{J. Lee}
\cite{lee2008nearest} offers a new approach to the analysis of Greek
morphology by using machine learning methods. The method proposed
relies on large amounts of data and makes use of nearest-neighbour
analysis techniques. This stands in contrast to the traditional
rule-driven approach used by earlier parsers such as
\cite{packard1973computer} or \cite{crane1991generating}. 

By applying affix transformations to word forms in order to analyse
their relation to other forms, a nearest-neighbour metric is
established on the basis of the number of these transformations needed
to generate another extant word form. An annotated corpus is fed to
the training architecture supported by a large amount of unlabelled data to
facilitate the prediction of verbal stems.

An accuracy of 98.2\% is achieved for words present in the training
set, with the remaining forms necessitating contextual
disambiguation. For words not present in the training set, an average
accuracy of 85.7\% is achieved with most of the loss in accuracy due
to words with most inaccuracies due to what the author terms `novel
roots', which are stems which are not directly derivable from a word
form and are analysed with a practical 50\% accuracy (though this was
improved to 65\% by loosening the standards by which accuracy was
measured).

The use of machine learning techniques in this paper is laudable and
certainly not badly executed, but the general methodology and in
particular the choice of training corpora is problematic. An annotated
version of the first five books Septuagint is used to establish these
metrics. This corpus consists of 470K words and can be reduced to a
set of about 37K unique words. Yet it is unnecessary to restrict
training to such a corpus if one does not look at n-grams but only at
stand-alone word forms. The question, then, is: why not use the output
of Morpheus, the system designed by Gregory Crane and honed over the
years, as training material? It is freely available as an SQL database
and contains \textasciitilde1M unique parses which are generated using very large
corpora for verification. The nearest-neighbor metric may be applied
to these word forms and arguably form a better picture, as well as
allowing for better extrapolation to data not present in the training
corpus.

Despite this glaring flaw, the applied method is still laudable in the
sense that it shows that it is not necessary to create very complex
rulesets for morphological analysis. With minimal integration of
linguistic knowledge, we can create a simple heuristic which offers
reasonable results.

\cite{conf/lrec/LeeH10}


\subsection{Collaborative / open-source philology}

\subsubsection{D. Bamman}
\cite{bammanpbml2008,bammantlt8,bamman2008building,bammandhq2009,bammancrane2011}

\subsubsection{G. Crane; the Open Philology Project}

