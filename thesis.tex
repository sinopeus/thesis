\documentclass[10pt,a4paper,twoside,openright,titlepage,fleqn,%
               headinclude,,footinclude,BCOR5mm,%
               numbers=noenddot,cleardoublepage=empty,%
               tablecaptionabove]{scrbook}
\usepackage[dutch,polutonikogreek,british]{babel}
\usepackage[utf8]{inputenc}
\usepackage[LGRx,T1]{fontenc}
\usepackage[final]{microtype}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{varioref}
\usepackage[style=philosophy-modern,hyperref,backref,square,natbib,ibidtracker=false]{biblatex}
\usepackage[tight,british]{minitoc}
\usepackage{wrapfig}
\usepackage{chngpage}
\usepackage{calc}
\usepackage{mflogo}
\usepackage{caption,listings,graphicx,subfig}
\usepackage{multicol}
\usepackage{makeidx}
\usepackage{xspace}
\usepackage{mparhack}
\usepackage{fixltx2e}
\usepackage{relsize}
\usepackage{lipsum}
\usepackage[eulerchapternumbers,subfig,beramono,eulermath,pdfspacing,listings]{classicthesis}
\usepackage{guit}
\usepackage{arsclassica}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{setspace}

\input{arsclassica-preamble}

\newcommand{\Greek}{\fontencoding{LGR}\selectfont}
\newcommand{\Latin}{\fontencoding{T1}\selectfont}
\newcommand\fullsc[1]{\scalebox{1.06}[1.09]{\textsc{#1}}}

\begin{document}
\pagestyle{plain}
\dominitoc

%******************************************************************
% front matter
%******************************************************************
\frontmatter
\input{frontbackmatter/titlepage}
\input{frontbackmatter/titleback}
\clearpage
\input{frontbackmatter/abstract}
\pagestyle{scrheadings}
\input{frontbackmatter/preface}
\input{frontbackmatter/acknowledgements}
\clearpage
\input{frontbackmatter/contents}
\incrementmtc
\incrementmtc
\cleardoublepage

%******************************************************************
% main matter
%******************************************************************
\mainmatter
\pagenumbering{arabic}

%************************************************
\chapter{Introduction and preliminaries}
\label{chp:introduction}
%\minitoc\mtcskip
%************************************************

The study of the language of the papyri has in the past thirty years
seen little evolution until the recent appearance of Evans and Obbink's
\textit{The Language of the Papyri} \citep{lpapyri}, which has placed
the subject in the spotlight again. Twentieth-century scholarship on the
topic, though still useful for those interested in the study of the
papyri for historical purposes, is either antiquated, limited in scope
or incomplete (see \textit{infra}). Despite this, the papyri are
useful source material for the history and evolution of the Greek
language, as they contain not only official texts but private
documents as well, whose linguistic features and peculiarities have the
potential to foster new insights into the nature of colloquial Greek. 

\section{Thesis}
The following thesis intends to prove that it is possible to generate
basic linguistic annotation for a large digitalised corpus of papyri in
ancient Greek using readily available tools and techniques with minimal
technical overhead. Such a corpus could be  a boon to scholars interested
in the Greek of the papyri, as it would facilitate, for instance, the
creation of linguistically sound grammars and lexica.

\section{Preliminaries}

The following section will provide a background sketch, consisting of a short
overview of previous efforts and an elucidation of some key concepts.

\subsection{The language of the papyri}

The papyri began to be studied linguistically not by papyrologists and
historians, but rather by Bible scholars and grammarians interested in their
relevance in the development began to koin\^{e} Greek, particularly that of the
New Testament. G. N.  Hatzidakis, W. Cr\"onert, K. Dieterich, A. Deissmann, and
A.  Thumb pioneered the field in the late nineteenth and early twentieth
century, spurring a resurgence of scholarship on the topic; an excellent
overview of pre-1970s research may be found in \citet{mandilaras1973} and
\citeauthor{gignac1976} [\citeyear{gignac1976} and \citeyear{gignac1981}].

During this period, Mayser begon work on the earliest compendious grammar of
the papyri; it limits itself to the Ptolemaic era but explores it at length and
in great detail.  The work consists of a part on phonology and morphology, made
up of three slimmer volumes, and a part on syntax, encompassing three larger
volumes. Its composition seems to have been exhausting: it took Mayser
thirty-six years to finish volumes I.2 through II.3, with I.1 only completed in
1970 by Hans Schmoll, at which point the entire series was given a second
edition.

When casually browsing through some of its chapters (though casual is hardly
the word one would associate with the \textit{Grammatik}) it is remarkable to see
that Mayser brings an abundance of material to the table for each grammatical
observation he makes, however small it may be. For instance, the section on
diminutives essentially consists of pages upon pages of examples categorised by
their endings.

This is its great strength as a reference work - whenever one is faced with an
unusual grammatical phenomenon in any papyrus, consulting Mayser is bound to
clarify the matter; or rather, it was, for the work is now inevitably dated.
The volumes published during Mayser's lifetime only include papyri up to their
date of publication; only the first tome by Schmoll includes papyri up to 1968.
It is still a largely useful resource, but it is in urgent need of refreshment.

After Mayser set the standard for the Ptolemaic papyri, a grammar of the
post-Ptolemaic papyri was the new \textit{desideratum} in papyrology. The work
had been embarked on by Salonius, Ljungvik, Kapsomenos, and Palmer, only to be
interrupted or thwarted by circumstance or lack of resources.
\citet{salonius1927}, for instance, only managed to write an introduction on
the sources, though he offered valuable comments on the matter of deciding how
close to spoken language a piece of writing is. \citet{ljungvik1932} contains
select studies on some points of syntax.

It is in the 1930's that we see attempts to create a grammar of the papyri that
would be the equivalent of Mayser for the post-Ptolemaic period.
\citeauthor{kapsomenos1938} published a series of critical notes
[\citeyear{kapsomenos1938}, \citeyear{kapsomenos1957}] on the
subject; though he attempted at a work on the scale of the \textit{Grammatik},
he found the resources sorely lacking, as the existing editions of papyrus
texts could not form the basis for a systematic grammatical study. The other
was \citeauthor{palmer1934}, who had embarked on similar project and had
already set out a methodology [\citeyear{palmer1934}]; the war interrupted his
efforts, and he published what he had already completed, a treatise on the
suffixes in word formation [\citeyear{palmer1945}].

A new work of some magnitude presents itself two decades later with B. G.
Mandilaras' \textit{The verb in the Greek non-literary papyri}
[\citeyear{mandilaras1973}]. Though it does not aim to be a grammar of the
papyri, it does offer a thorough and satisfactory treatment of the verbal
system as manifest in the papyri.  Further efforts essentially do not appear
until the publication of Gignac's grammar. It is essentially treading in the
footsteps of Mayser, only with further methodological refinement and a more
limited, though still sufficiently exhaustive, array of examples. The author,
for reasons unknown to me, only managed to complete two of the three projected
volumes, on phonology and on morphology. The volume on syntax is thus absent, a
gap only partly filled by Mandilaras' \textit{The verb in the Greek
non-literary papyri}.

Finally, there is the aforementioned \textit{The Language of the Papyri}
\citep{lpapyri}, which does not aim to be a work on the same scale as the
aforementioned. It is a collection of articles on various topics, the whole of
which is meant to illuminate new avenues for future research. A particularly
relevant chapter for this thesis is the last one by Porter and O'Donnell
\citep{porter2010}, who set out to create a linguistic corpus for a selection
of papyri; their tagging approach, however, is manual, and their target corpus
limited. The authors also are the creators of \url{http://www.opentext.org/}, a
project aiming for the development of annotated Greek corpora and tools to
analyse them; sadly, no progress seems to have been made since 2005.

\subsection{Corpus linguistics}
A\footnote{The following section is based \emph{passim} on
\citet{okeeffe2010}.} corpus or text corpus is a large, structured collection
of texts designed for the statistical testing of linguistic hypotheses. The
core methodological concepts of this mode of analysis may be found in the
concordance, a tool first created by biblical scholars in the Middle Ages as an
aid in exegesis. Among literary scholars, the concordance also enjoyed use,
although to a lesser degree; the eighteenth century saw the creation of a
concordance to Shakespeare.

 The development of the concordance into the modern corpus was not primarily
 driven by the methods of biblical and literary scholars; rather, lexicography
 and pre-Chomskyan structural linguistics played a crucial role.

 Samuel Johnson created his famous comprehensive dictionary of English by means
 of a manually composed corpus consisting of countless slips of paper detailing
 contemporary usage. A similar method was used in the 1880s for the Oxford
 English Dictionary project - a staggering three million slips formed the basis
 from which the dictionary was compiled.

 1950s American structuralist linguistics was the other prong of progress; its
 heralding of linguistic data as a central given in the study of language
 supported by the ancient method of searching and indexing ensures its
 proponents may be called the forerunners of corpus linguistics.

Computer-generated concordances make their appearance in the late 1950s,
initially relying on the clunky tools of the day - punch cards. A notable
example is the \emph{Index Thomisticus}, a concordance
to the works of Thomas of Aquino created by the late Roberto Busa S.J. which
only saw completion after thirty years of hard work; the printed version spans
56 volumes and is a testament to the diligence and industry of its author. The
1970s brought strides forward in technology, with the creation of computerised
systems to replace catalogue indexing cards, a change that greatly benefited
bibliography and archivistics.

 It is only in the 1980s and 1990s that are marked the arrival of fully
 developed corpora in the modern sense of the word; for though the basic
 concepts of corpus linguistics were already widely used, they could not be
 applied on a large scale without the adequate tools. The rise of the desktop
 computer and the Internet as well as the seemingly ever-rising pace of
 technological development ensured the accessibility of digital tools.  The old
 tools - punch cards, mainframes, tape recorders and the like - were gladly
 cast aside in favour of the new data carriers.

 The perpetual increase of computing power equally demonstrated the limits of
 large-scale corpora; while lexicographical projects that had as their purpose
 to document the greatest number of possible usages could keep increasing the
 size of their corpora, the size of others went down as they whittled the data
 down to a specific set of uses of language.

 The possible applications of the techniques of corpus linguistics are diverse
 and numerous; for they allow for a radical enlargement in scope while
 remaining empirical, and remove arduous manual labour from the equation.
 Corpus linguistics can be an end to itself; it can, however, assert an
 important role in broader research.  \citet[7]{okeeffe2010} mention areas such
 language teaching and learning, discourse analysis, literary stylistics,
 forensic linguistics, pragmatics, speech technology, sociolinguistics and
 health communication, among others.

The term `corpus' has a slightly different usage in classical philology: they
designate a structured collection of texts, but that collection is not
primarily intended for the testing of linguistic hypotheses. Instead, we have,
for instance, the ancient corpus Tibullianum, or modern-day collection, for
  instance the Corpus Papyrorum Judaicarum, etc. We are primarily interested in
  the digital techniques used to create linguistic corpora; so let us first
  take a look at the progress of the digital classics.


\subsection{The digital classics} 

Classical philology, despite its status as one of the oldest and most
conservative scientific disciplines still in existence today, has in the last
fifty years found itself at the front lines of the digital humanities movement.
Incipient efforts in the fifties and sixties, mainly stylometric and lexical
studies and  the development of concordances, demonstrated the relevance of
informatics in the classics, an evolution that was at first met with some
skepticism, but later fully embraced.

The efforts began with the aforementioned Index Thomisticus, the first
computer-based corpus in a classical language; but the first true impetus was
the foundation of the Thesaurus Linguae Graecae project in 1972, a monumental
project with as its goal the stocking of all Greek texts from the Homeric epics
to the fall of Constantinople. Over the years, many functions have been added
to this ever more powerful tool; and even in the beginning stages of its
development, the TLG garnered praise.

The usefulness of the tool in its current form cannot be overstated: not only
does it contain a well-formatted and easily accessible gigantic collection of
text editions whose scope and dimensions exceed those of nearly any university
library; it also offers all of these texts in a format that allows for lexical,
morphological and proximity searches, as well as including a full version of
the Liddell \& Scott and Lewis \& Short dictionaries. The TLG has become a
staple of the digital classics.

Despite this, the TLG is becoming more and more dated as technology progresses.
While recent years have seen the rise of Unicode as the standard for encoding
ancient Greek, the TLG still uses beta code, a transliteration system designed
to only use the ASCII character set, and the texts are stored using an obsolete
text-streaming format from 1974, which divides the text in blocks of eight
kilobytes and marks the division between segment. 

A digitised version of the Liddell-Scott-Jones lexicon has been added to the
TLG's web interface, but the texts themselves have not undergone extensive
tagging, only lemmatisation.  Searching through the database can be done by
searching for specific forms of a lemma, or by searching for all forms of a
lemma, but this is essentially the limit of the search tool's power; it is not
possible to perform a query for all possible lemmata associated with a
particular form, i.e.\ we cannot find all forms which are, for example, an
active perfect indicative.

In the wake of the TLG, several notable projects have emerged: Brepols' Library
of Latin Texts is trying hard to be for Latin texts what the TLG is for Greek
texts; the Packard Humanities Institute has released CD's containing a
selection of classical Latin works. In more recent times, the Perseus Project
has enjoyed great popularity because of the attractive combination of an
excellent selection of classical texts with translations, good accessibility
and a set of interesting textual tools, the entire package carrying a very
interesting price tag for the average user — it is free to use, and for the
greatest part, open source as well.

The databases I have mentioned are quite general in scope; but within the
domain of classical philology, other specialised projects exist. Within the
field of papyrology, for instance, the digital revolution has taken a firm
foothold. Starting with several separate databases, the field has experienced a
tendency towards convergence and integration of the available resources, as
exemplarised by the papyri.info website, maintained by Columbia University,
that integrates the main papyrological databases into a single database.

A great feature of this database is the shell in which all data is wrapped;
they are compliant with the EpiDoc standard, a subset of XML based on the TEI
standard and developed specifically for epigraphical and papyrological texts.
One may access the database’s resources through the Papyrological Navigator and
suggest corrections and readings through the Papyrological Editor. What’s more,
all data is freely accessible under the Creative Commons License,
crowd-sourced, regularly updated, and can be downloaded for easier searching
and tweaking.

In other words, papyri.info has brought the open-source mentality from the
computer world into the classics. For our purposes, this open setup is
desirable, as the database is not fit for them as it is, but can with some
effort be molded into a useful tool.

\subsection{Natural language processing} 

Natural language processing (henceforth NLP) is a subdiscipline in computer
science concerned with the interaction between natural human language and
computers. Its history well and truly starts in the fifties, with a basic
concept which has played a great role in natural language processing, and
computer science in general, the Turing test. This test, put forth by Alan
Turing in his seminal paper \textit{Computing Machinery and Intelligence}
\citep{turing1950}, evaluates whether a machine is intelligent or not by
placing a human in conversation with another human and a machine; if the first
human cannot tell the other human and the machine apart, the machine passes the test.

Machine translation systems entered development, though progress soon stalled
because of technical limitations and because of methodological obstacles: such
systems were dependent on complex rulesets written by programmers that allowed
for very little flexibility. Because of the slow return on investments made,
funding for artificial intelligence in general and machine translation
specifically was drastically reduced throughout the late sixties and the seventies.

A resurgence followed: thanks to advances in computational power and the
decline of Chomskyan linguistics, which had been the dominant theoretical
vantage point in the preceding thirty years, the eighties were marked by the
introduction of statistical machine translation, which is fundamentally based
on the tenets of corpus linguistics. Modern natural language processing is
therefore situated on the crossroads between various fields: artificial
intelligence, computer science, statistics, and corpus and computational
linguistics. It looks to be an exciting field for the coming years as its
techniques are under constant improvement and ever more present in our daily lives.

Most NLP software is designed explicitly with living languages in mind;
English, being a world language and the international \textit{lingua franca},
has enjoyed most of the attention, but other major languages have enjoyed some
attention, too. Ancient languages, however, are neglected, presumably due to
their often high complexity and the extensive study and analysis to which they
have been submitted by skilled scholars. Yet most texts have not been
integrated in annotated corpora; and though databases such as the Perseus
project contain large swathes of morphologically and sometimes syntactically
annotated text, the process has been driven largely by manual labour; to give
an exhaustive list is not appropriate here, but another such example which is
relevant is the PROIEL project, which is also a treebank, i.e.\ a database of
syntactically annotated sentences. It contains data for Herodotus and the New Testament.

On the other hand, there are also corpora which have been tagged using NLP
techniques, whose relevancy for this thesis is high and that I have thus
described in the next section on methodology.

\section{Methodology}

In this thesis, we have been largely inspired by two articles by H. Dik and R.
Whaling, [\citeauthor{dik2008}, \citeyear{dik2008} and \citeyear{dik2009}], in
which they document their method for semi-automatically tagging the Perseus
project's texts under their own framework, PhiloLogic. They start with a
database of analysed forms and a series of tagged texts which they use as
initial data to train a decision tree tagger, TreeTagger, developed by Helmut
Schmid at the University of Stuttgart, a tool which despite being developed in
1995 has aged well as far as performance is concerned. They achieved remarkable
accuracy: with refinements to the training data they achieved 96.2\% accuracy
during tests on the original training data and 91\% accuracy on new data, a
result which compares quite favorably when compared to TreeTagger's 97\%
accuracy when used on German newspaper articles considering the high complexity
of ancient Greek and the variety of styles of ancient Greek literature.

It occurred to me that this might be a great method for processing the
papyri.info database with a relatively small effort for a high payoff; using
data from the Perseus and PROIEL projects, it could be possible to train
TreeTagger for both morphology and syntax, apply the resulting parameters to
the corpus and thus for the greatest part obviate the need for manual tagging.
Given the extent of the corpus (about 50,000 texts containg almost 4,500,000
words), achieving even 85\% accuracy would reduce the amount of untagged words
to 675,000, many of which I would expect to be proper names or morphologically
`erroneous' forms as are often found in the papyri, data which could itself be
analysed with regular expressions and then used to improve the training
data.\footnote{As I set out to verify the originality of my thesis, I found
  that this statistical approach has been used before for textual criticism!
  \textit{Vide} \citet{mimno2009}, an abstract of which may be found at
  \url{http://people.cs.umass.edu/~wallach/publications/mimno09computational.txt}.}

%************************************************
\chapter{The corpus}
\label{chp:design}
\minitoc\mtcskip
%************************************************
\section{Goals}

Our objectives for the modified corpus is threefold. Firstly, we want to
morphologically annotate it with reasonable accuracy; secondly, we also want to
add syntactical annotation; thirdly, we want to ensure the corpus is compatible
with a broad range of tools.

The former two goals are the central enterprise of this thesis. Morphological
annotation will pose little theoretical problems, as the morphological features
of Greek are not subject to discussion; but for syntactical annotation, a
variety of theoretical frameworks are in existence. The fact of the matter is
that I have chosen to take the route which is technically easiest: to use the
tenets of dependency grammar. The reason Leuven : K.U.Leuven. Faculteit Letteren, 2011 is twofold: on one hand, our training
data, the New Testament annotated by PROIEL, itself is structured using a
dependency model; on the other hand, dependency trees, in contrast to
constituent analysis, assigns each word or morph one node, which is easier to
`digest' for a computer program.

Our third goal is to format the corpus in a way that will ensure broad
compatibility. This is desirable for several reasons. A first is that a broadly
used format offers many options for data treatment; for instance,
comma-separated text files can be read by many programs, from the simplest text
editor to the most complex software packages, while XML can very easily be
integrated in a web interface or parsed and transformed by a variety of
programs. A second is that corpus is to be published on GitHub for
collaborative editing, or possibly pulled into the main papyri.info repository.
This depends on our first point: the idea of GitHub is that all projects are
open source and can be contributed to by others, which is facilitated by using a
format with wide adoption.

\section{Design}
\subsection{Initial inquiries}

Before arriving at the approach described above, I attempted several other
possible methods for automatic analysis. A first idea was to integrate the XML
files in a PhiloLogic setup.  PhiloLogic is a tool developed by the ARTFL
project and the Digital Library Development center at the University of Chicago
and released under the GNU General Public License. Its original purpose was to
serve as the full-text search, retrieval and analysis tool for large databases
of French literature.

PhiloLogic has support for TEI XML and boasts an impressive
array of features: it can search through text and deliver KWIC results
filtered by frequency and metadata, as well as collocation tables and word
order information.  Furthermore, the development web page cites support for
bibliographic backends in MySQL databases, out of the box operation,
interoperability across Unix-based systems, etc.

The tool seemed promising; especially the built-in support for XML and
Unicode drew my attention, along with the built-in features for linguistic
analysis. It essentially would have made my work easier and allowed me to
invest more time in investigating a few linguistic topics. In the end, however,
the software did not fit my needs in a few respects.

Firstly, PhiloLogic requires Apache, Perl and several CPAN modules, as well as
gawk, gdbm, and agrep.  Not a huge amount of dependencies, but still less than
using Python and the NLTK, which does not require a server or SQL database to
run; also a big problem is the out-of the box incompatibility of the required
CPAn  modules with 64-bit systems. Setting PhiloLogic up was not as
easy as advertised; I had to resort to a virtual machine running 32-bit Debian
Linux until I was able to discover a method that enabled compatibility with
Mac OS X 10.7, my OS of choice.

Secondly, XML support includes standard TEI  but has issues with EpiDoc; notably,
in my experience, headers were treated as text and EpiDoc's complicated tagset
was incorrectly rendered; for instance, original readings and corrections were
concatenated in the text browser, an undesirable situation if we wish to have
any hand in our choice of corpus, and a possible source of statistical errors.
Developing brand new XSLT stylesheets to convert EpiDoc to a simpler form of
TEI markup or to raw text could perhaps have been a feasible solution; but
PhiloLogic seemed to behave erratically in general and a first attempt at
feeding the system raw text (advertised as one of its capabilities) turned out
a spathe of errors.

Thirdly, PhiloLogic's feature set is extensive, but I soon came to realize that it
does not offer much more than papyri.info already does; it is essentially a
robust concordancing application through a web interface. I wished not only to
have a lemmatised corpus searchable by word proximity, but also by word
relations, which would shed more light on the syntax of the Greek of the
papyri, as this is arguably the largest gap in scholarship due to the age of
Mayser's \emph{Grammatik} and the incomplete state of Gignac's \emph{Grammar}.
Furthermore, the tool as available to the public does not integrate the
morphological database of Greek developed for Perseus under PhiloLogic --- a
regrettable point, really, since it was exactly this capacity I wished to
exploit in the first place. Unicode search for Greek also seems to be in need
of improvement; the search form still requires transcription. Modifying the
tool would have been unfeasible given the size of the source code.

Thus I discarded PhiloLogic from my options and set out to find or develop a
simpler system.\footnote{I also considered setting up a mirror of papyri.info
on a personal server and modifying the search functionality; this turns out not
to be a task for the weak-hearted. The required setup causes substantial
overhead for our purposes, too much to be a reasonable solution.} After the
technical struggle with PhiloLogic, I decided to focus my efforts on
concordancers that could replicate its functions. A quick consultation of
Stanford's list of resources \citep{stanfordnlpres} for statistical natural
language processing and corpus-based computational linguistics directed me to
WordSmith Tools, which is regrettably Windows-only and a commercial, closed
source program to boot.  I set out on a search for open-source alternatives and
found a diversity of programs, most of them without the same functionality as
WordSmith, and invariably clunky or antiquated --- making WordSmith the only
option, but even that did not seem viable for my ends.

I continued my search and stumbled across a very interesting piece of software
that is enjoying a good amount of popularity as a didactic tool for
computational corpus linguistics; the Python NLTK, short for Natural
Language Tool Kit, is not a stand-alone application, but rather a set of ``open
source Python modules for research and development in natural language
processing and text analytics bundled with data and documentation''
\citep{nltkhome}. These modules implement diverse functionalities useful for
natural language processing and allow their easy integration into Python
programs. The Python NLTK  seemed to offer a more than solid amount of
features: corpus readers, tokenizers, stemmers, taggers, chunkers, parsers,
classifiers, clusterers, tools for semantic interpretation and metrics were all
integrated from the get-go and are easily combined and extended. The
NLTK was developed for English, but its extensibility meant it could
be applied to ancient Greek with relative ease. A further advantage is the fact
that Python is very well-suited to text manipulation and parsing XML,
which is an advantage when working with the papyri.info corpus; it is also
cross-platform: Python interpreters are available for all major platforms.

Now that I had a solid tool for interpreting corpora, the only issue remaining
was to construct a linguistically annotated corpus out of the papyri.info
corpus. The extent of the corpus makes extensive manual tagging by a single
person unfeasible; thus I set out to look for an automated tagger which could
be used, immediately or with some effort,  on ancient Greek. A variety of
taggers seem to be available, but I settled on TreeTagger, developed by Helmut
Schmid, for several reasons:

\section{Technical} % (fold)
\label{sec:corpus-technical}

\subsection{Requirements} % (fold)
\label{sub:requirements}

To create the annotated corpus as I have, it is necessary to have the following installed (older versions may work but have not been tested):

\begin{itemize}
  \item a UNIX-like operating system, i.e. Mac OS X, any variety of Linux or BSD (using Windows should also be possible, since all tools used are portable and cross-platform, but the file location hierarchies in some scripts will not work);
  \item Python 2.7.3, found at \url{http://www.python.org/} (do not use Python 3.x, the NLTK is not compatible with it);
  \item Perl 5.x \url{http://www.perl.org/};
  \item Saxon-HE 9.x, found at \url{http://saxon.sourceforge.net/};
  \item Helmut Schmid's TreeTagger, found at \url{http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/};
  \item the Python Natural Language Toolkit (NLTK), found at \url{http://nltk.org/};
  \item SQLite 3, found at \url{http://www.sqlite.org/}.
\end{itemize}

All the above, excluding Mac OS X, are freely available.

% subsection Requirements (end)

\subsection{Extracting training data} % (fold)
\label{sub:extract_training_data}

% subsection Extracting training data (end)

\subsection{Principles of statistical natural language processing} % (fold)
\label{sub:principles-nlp}

This subsection is intended as an overview of some key concepts on which
natural language processing is based; it is based mainly on 
\citet{manning1999}.

\subsubsection{Formal languages} % (fold)
\label{sub:formallang}

% subsection Formal languages (end)

\subsubsection{Hidden Markov models} % (fold)
\label{ssub:markovchains}

% subsubsection Markov chains (end)

\subsubsection{Zipf's law} % (fold)
\label{ssub:zipflaw}
% subsubsection Zipf's law (end)

% subsection Principles of statistical natural language processing (end)

\subsection{Accuracy testing} % (fold)
\label{sub:accuracy-testing}

% subsection Accuracy testing (end)

% section Technical (end)
%************************************************
\chapter{The tool}
\label{chp:design}
\minitoc\mtcskip
%************************************************
\section{Goals} % (fold)
\label{sec:tool-goals}

% section Goals (end)

\begin{enumerate} 
  \item The tool must be able to read EpiDoc XML and a subset of
      additional tags added for linguistic annotation. It should be,
      essentially, a text-based replica of the Papyrological Navigator as found
      at \texttt{papyri.info}. 
    \item search 
    \item statistics
    \item syntactic trees
\end{enumerate}

\section{Design} % (fold) 
\label{sec:tool-design} 
The tool works from the command line; while it has the obvious disadvantage of
being a type interface most people cannot work with right off the bat, command
line programs generally have an edge over programs with a graphical user
interface: they are smaller, faster, and more efficient, and in Unix
environments can easily be integrated in a larger workflow.
% section Design (end)

\section{Technical} % (fold)
\label{sec:tool-technical}
The tool is written in Python 3. 
% section Technical (end)

\section{Interface} % (fold)
\label{sec:interface}

The basic command for launching the script is \texttt{python tjufy} from within
a terminal or simply \texttt{tjufy} from within the Python interpreter (from
here on I will simply write \texttt{tjufy}). Running it without options like this
will display a help dialog; the same goes for running \texttt{python tjufy -h}
or \texttt{python tjufy --help}.


% section User interface (end)

\chapter{Applications} % (fold)
\label{cha:applications}

\section{Collaborative editing} % (fold)
\label{sec:collaborativeediting}

% section Collaborative editing (end)

\section{Corpus-based grammars} % (fold)
\label{sec:corpusbasedgrammars}

% section Corpus-based grammars (end)

\section{Textual criticism} % (fold)
\label{sec:textualcriticism}

% section Textual criticism (end)

% chapter Applications (end)

\chapter{Conclusion} % (fold)
\label{cha:conclusion}

% chapter Conclusion (end)

% *****************************************************************
% back matter
%******************************************************************
\backmatter
\clearpage
\input{frontbackmatter/bibliography}
%\clearpage
%\input{frontbackmatter/appendix}
\end{document}
